{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Data Science Training</center>\n",
    "<center><b>Cross Validation Template</b><br>\n",
    "Jiannan, 2016</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,average_precision_score,roc_curve,auc,precision_recall_curve\n",
    "from sklearn.metrics import log_loss\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from scipy import interp\n",
    "\n",
    "import random\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define fucntions to calculate feature importances in different criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_importance(_bst, _importance_type):\n",
    "    # if it's weight, then omap stores the number of missing values\n",
    "    fmap = ''\n",
    "    if _importance_type == 'weight':\n",
    "        # do a simpler tree dump to save time\n",
    "        trees = _bst.get_dump(fmap, with_stats=False)\n",
    "\n",
    "        fmap = {}\n",
    "        for tree in trees:\n",
    "            for line in tree.split('\\n'):\n",
    "                # look for the opening square bracket\n",
    "                arr = line.split('[')\n",
    "                # if no opening bracket (leaf node), ignore this line\n",
    "                if len(arr) == 1:\n",
    "                    continue\n",
    "\n",
    "                # extract feature name from string between []\n",
    "                fid = arr[1].split(']')[0].split('<')[0]\n",
    "\n",
    "                if fid not in fmap:\n",
    "                    # if the feature hasn't been seen yet\n",
    "                    fmap[fid] = 1\n",
    "                else:\n",
    "                    fmap[fid] += 1\n",
    "\n",
    "        return fmap\n",
    "\n",
    "    else:\n",
    "        trees = _bst.get_dump(fmap, with_stats=True)\n",
    "\n",
    "        _importance_type += '='\n",
    "        fmap = {}\n",
    "        gmap = {}\n",
    "        for tree in trees:\n",
    "            for line in tree.split('\\n'):\n",
    "                # look for the opening square bracket\n",
    "                arr = line.split('[')\n",
    "                # if no opening bracket (leaf node), ignore this line\n",
    "                if len(arr) == 1:\n",
    "                    continue\n",
    "\n",
    "                # look for the closing bracket, extract only info within that bracket\n",
    "                fid = arr[1].split(']')\n",
    "\n",
    "                # extract gain or cover from string after closing bracket\n",
    "                g = float(fid[1].split(_importance_type)[1].split(',')[0])\n",
    "\n",
    "                # extract feature name from string before closing bracket\n",
    "                fid = fid[0].split('<')[0]\n",
    "\n",
    "                if fid not in fmap:\n",
    "                    # if the feature hasn't been seen yet\n",
    "                    fmap[fid] = 1\n",
    "                    gmap[fid] = g\n",
    "                else:\n",
    "                    fmap[fid] += 1\n",
    "                    gmap[fid] += g\n",
    "\n",
    "        # calculate average value (gain/cover) for each feature\n",
    "        for fid in gmap:\n",
    "            gmap[fid] = gmap[fid] / fmap[fid]\n",
    "\n",
    "        return gmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files and preparation before cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_modelA_clean = pd.read_csv(\"input_file_path\",sep=\"|\",na_values=[\"\\N\", \"NULL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_TARGETS = ['target_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeting_features_file = 'feature_columns_metadata_path'\n",
    "feature_df_targeting = pd.read_csv(targeting_features_file,na_values=[\"\\N\",\"NULL\"])\n",
    "\n",
    "features_columns_targeting = feature_df_targeting.targeting_features.tolist()\n",
    "\n",
    "print(len(features_columns_targeting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data set for targeting\n",
    "dataset_modelA_clean_targeting = dataset_modelA_clean[np.concatenate([features_columns_targeting, ALL_TARGETS])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ALL_TARGETS)\n",
    "\n",
    "for target in ALL_TARGETS:\n",
    "    print(\"target number %s\"%target)\n",
    "    print(dataset_modelA_clean_targeting[target].sum())\n",
    "    print(\"target proportion %s\"%target)\n",
    "    print(dataset_modelA_clean_targeting[target].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start CV for Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_cross_validation(_df, _classifier, _features_columns):\n",
    "    # cross validation type can be changed here\n",
    "    ss = sk.cross_validation.ShuffleSplit(len(_df.svocmasterid.unique()), n_iter=3, test_size=.3, random_state=0)\n",
    "    target='target_all'\n",
    "    prob_of = 'prob_of_all'\n",
    "    \n",
    "    results_cv_targeting = pd.DataFrame([], columns=['svocmasterid', target, 'fold', prob_of])\n",
    "\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    mean_lift = []\n",
    "    mean_tp = 0.0\n",
    "    mean_fp = range(0, 101)\n",
    "\n",
    "    nb_calls_cv = pd.DataFrame([],columns=['nb_contacts', 'total_population', 'total_pos_targets', 'nb_pos_targets', 'pos_rate', \n",
    "                                           'Percentage_of_pos_targets_found', 'Percentage_of_Population', 'Lift'])\n",
    "    feature_importances = pd.DataFrame([], columns=['feature', 'importance', 'fold'])\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 12))\n",
    "    fig.subplots_adjust(bottom=-0.5, left=-0.5, top=0.5, right=1.5)\n",
    "\n",
    "\n",
    "    print ('modeling started')\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(ss):\n",
    "\n",
    "        customer_id = _df.svocmasterid.unique().copy()\n",
    "        shuffled_customer_id = np.array(sorted(customer_id, key=lambda k: random.random()))\n",
    "        train_customer_id = shuffled_customer_id[train_index]\n",
    "        valid_customer_id = shuffled_customer_id[valid_index]\n",
    "\n",
    "        train = _df.loc[ _df.svocmasterid.isin(train_customer_id), np.concatenate([_features_columns, [target]],\n",
    "                        axis=0)].copy().reset_index(drop=True)\n",
    "        valid = _df.loc[_df.svocmasterid.isin(valid_customer_id), np.concatenate([_features_columns, [target]],\n",
    "                        axis=0)].copy().reset_index(drop=True)\n",
    "\n",
    "        temp = valid[['svocmasterid', target]].copy()\n",
    "        temp['fold'] = i\n",
    "\n",
    "        # modeling#\n",
    "        train_X = train.drop(['svocmasterid', target], axis=1)\n",
    "        valid_X = valid.drop(['svocmasterid', target], axis=1)\n",
    "\n",
    "        train_Y = np.array(train[target].astype(np.uint8))\n",
    "        valid_Y = np.array(valid[target].astype(np.uint8))\n",
    "\n",
    "        probas_ = _classifier.fit(train_X, train_Y,eval_metric='auc', eval_set=[(valid_X, valid_Y)],early_stopping_rounds=40).predict_proba(valid_X) \n",
    "        probabilities = pd.DataFrame(data=probas_[:, 1], index=valid_X.index, columns=[prob_of])\n",
    "\n",
    "        temp = temp.join(probabilities, how='left')\n",
    "        results_cv_targeting = results_cv_targeting.append(temp)\n",
    "\n",
    "        ###############################################################################\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = sk.metrics.roc_curve(valid_Y, probas_[:, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = sk.metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(fpr, tpr, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        ###############################################################################\n",
    "        # compute lift at 10%#\n",
    "        sorted_proba = np.array(list(reversed(np.argsort(probas_[:, 1]))))\n",
    "        X_test = valid_X\n",
    "        y_test = valid_Y\n",
    "        centile = X_test.shape[0] / 100\n",
    "        positives = sum(y_test)\n",
    "        lift = [0]\n",
    "        for q in xrange(1, 101):\n",
    "            if q == 100:\n",
    "                tp = sum(np.array(y_test)[sorted_proba[(q - 1) * X_test.shape[0] / 100:X_test.shape[0]]])\n",
    "            else:\n",
    "                tp = sum(\n",
    "                    np.array(y_test)[sorted_proba[(q - 1) * X_test.shape[0] / 100:q * X_test.shape[0] / 100]])\n",
    "            lift.append(lift[q - 1] + 100 * tp / float(positives))\n",
    "        quantiles = range(0, 101)\n",
    "        mean_tp += interp(mean_fp, mean_fp, lift)\n",
    "        mean_tp[0] = 0.0\n",
    "        mean_lift.append(lift[10] / 10.)\n",
    "\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(quantiles, lift, label='Lift fold %d at 10 = %0.2f' % (i, lift[10] / 10.))\n",
    "        print ('shuffle: %i, AUC: %f, lift at 10 percent: %f' % (i, roc_auc, lift[10] / 10.))\n",
    "        \n",
    "        ###############################################################################\n",
    "        # Calculate nb contacts to make\n",
    "        nb_calls = temp[['target_all','prob_of_all','fold']].copy()\n",
    "        nb_calls = nb_calls.sort_values(by='prob_of_all', ascending=False).reset_index(drop=True)\n",
    "        nb_calls['cum_Xsellers'] = np.cumsum(nb_calls.target_all)\n",
    "        nb_calls = nb_calls.reset_index(drop=False)\n",
    "        nb_calls = nb_calls.rename(columns={'index':'rank'})\n",
    "        nb_calls['nb_contacts_100'] = nb_calls.loc[nb_calls.cum_Xsellers==100,'rank'].min()\n",
    "        nb_calls['nb_contacts_200'] = nb_calls.loc[nb_calls.cum_Xsellers==200,'rank'].min()\n",
    "        nb_calls['nb_contacts_500'] = nb_calls.loc[nb_calls.cum_Xsellers==500,'rank'].min()\n",
    "        nb_calls['nb_contacts_1000'] = nb_calls.loc[nb_calls.cum_Xsellers==1000,'rank'].min()\n",
    "        nb_calls['nb_contacts_2000'] = nb_calls.loc[nb_calls.cum_Xsellers==2000,'rank'].min()\n",
    "        nb_calls['nb_contacts_3000'] = nb_calls.loc[nb_calls.cum_Xsellers==3000,'rank'].min()\n",
    "        nb_calls['nb_contacts_all'] = nb_calls.loc[nb_calls.cum_Xsellers==nb_calls.cum_Xsellers.max(),'rank'].min()\n",
    "        nb_calls = nb_calls[['nb_contacts_100','nb_contacts_200', 'nb_contacts_500','nb_contacts_1000', 'nb_contacts_2000','nb_contacts_3000','nb_contacts_all']].min()\n",
    "        nb_calls = pd.DataFrame(nb_calls,columns=['nb_contacts'])\n",
    "        nb_calls['total_population'] = temp.shape[0]\n",
    "        nb_calls['total_pos_targets'] = temp.target_all.sum()\n",
    "        nb_calls['nb_pos_targets']=[100,200,500,1000,2000,3000, temp.target_all.sum()]\n",
    "        nb_calls['pos_rate'] = nb_calls.nb_pos_targets/nb_calls.nb_contacts\n",
    "        nb_calls['Percentage_of_pos_targets_found'] = nb_calls.nb_pos_targets/nb_calls.total_pos_targets\n",
    "        nb_calls['Percentage_of_Population'] = nb_calls.nb_contacts/nb_calls.total_population\n",
    "        nb_calls['Lift'] = nb_calls.Percentage_of_pos_targets_found/nb_calls.Percentage_of_Population\n",
    "\n",
    "        nb_calls_cv = nb_calls_cv.append(nb_calls)\n",
    "        \n",
    "        ###############################################################################\n",
    "        feature_importances_data = []\n",
    "        features = train_X.columns\n",
    "        for feature_name, feature_importance in get_importance(_classifier.booster(), 'gain').iteritems():\n",
    "            feature_importances_data.append({\n",
    "                'feature': feature_name,\n",
    "                'importance': feature_importance\n",
    "            })\n",
    "\n",
    "        temp = pd.DataFrame(feature_importances_data)\n",
    "        temp['fold'] = i\n",
    "        feature_importances = feature_importances.append(temp)\n",
    "        \n",
    "    \n",
    "    nb_calls_cv = nb_calls_cv.reset_index().groupby('index').mean().sort_values(by='nb_pos_targets')\n",
    "    results_cv_targeting = results_cv_targeting.reset_index(drop=True)\n",
    "    \n",
    "    feature_importances = feature_importances.groupby('feature')['importance'].agg([np.mean, np.std])\n",
    "    feature_importances = feature_importances.sort_values(by='mean')\n",
    "    feature_importances = feature_importances.reset_index()\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    mean_tpr /= len(ss)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = sk.metrics.auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6))\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    mean_tp /= len(ss)\n",
    "    mean_tp[-1] = 100.0\n",
    "    mean_lift10 = np.mean(mean_lift)\n",
    "    print('Mean AUC: %f, Mean lift at 10 percent: %f' % (mean_auc, mean_lift10))\n",
    "    plt.plot(mean_fp, mean_tp, 'k--', label='Mean lift at 10 = %0.2f' % mean_lift10, lw=2)\n",
    "\n",
    "    plt.plot([0, 100], [0, 100], 'k--', color=(0.6, 0.6, 0.6))\n",
    "    plt.xlim([-5, 105])\n",
    "    plt.ylim([-5, 105])\n",
    "    plt.xlabel('Percentage of population')\n",
    "    plt.ylabel('Cumulative gain')\n",
    "    plt.title('Lift', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return results_cv_targeting, feature_importances, nb_calls_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# parameters of the classifier need to be changed according to datasets\n",
    "classifier = xgb.XGBClassifier(objective='binary:logistic',max_depth=6,n_estimators=400, learning_rate=0.05,max_delta_step=1,\n",
    "                        min_child_weight=25, gamma=0.1, scale_pos_weight=1, colsample_bytree=0.85, subsample=0.85,colsample_bylevel=0.85,\n",
    "                        nthread=10, seed=27)\n",
    "\n",
    "results_cv_targeting, feature_importances, nb_calls_cv = run_cross_validation(dataset_modelA_clean_targeting, classifier , features_columns_targeting) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances_sort = feature_importances.sort_values(by='mean',ascending=False)\n",
    "\n",
    "feature_importances_sort['relative_imp'] = 100.0 * (feature_importances_sort['mean'] / feature_importance_sort['mean'].max())\n",
    "#feature_importances_sort['mean']/feature_importances_sort['mean'].sum()\n",
    "feature_importances_sort.to_csv('file_path_to_store_feat_imp',sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_top10 = feature_importances_sort[:10][::-1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "plt.title(\"Top10 Most Important Feature importances for Model\")\n",
    "plt.barh(feature_importances_top10.index, feature_importances_top10['relative_imp'],\n",
    "         color='#348ABD', align=\"center\", lw='3', edgecolor='#348ABD', alpha=0.6)\n",
    "plt.yticks(feature_importances_top10.index, feature_importances_top10['feature'], fontsize=12,)\n",
    "plt.ylim([-1, feature_importances_top10.index.max()+1])\n",
    "plt.xlim([0, feature_importances_top10['relative_imp'].max()*1.1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

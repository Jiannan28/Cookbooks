{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Data Science Training</center>\n",
    "<center><b>Model Creation Template</b><br>\n",
    "2016</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.externals import joblib\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_modelA_clean = pd.read_csv(\"input_path\",sep=\"|\",na_values=[\"\\N\", \"NULL\"]) #input file path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_TARGETS = ['target_all']\n",
    "\n",
    "targeting_features_file = 'feature_path' #'feature_columns_metadata_path' \n",
    "feature_df_targeting = pd.read_csv(targeting_features_file,na_values=[\"\\N\",\"NULL\"])\n",
    "\n",
    "feature_columns = feature_df_targeting.targeting_features.tolist()\n",
    "\n",
    "print(len(feature_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get data set for predicting\n",
    "dataset_modelA_clean_targeting = dataset_modelA_clean[np.concatenate([feature_columns, ALL_TARGETS])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = dataset_modelA_clean_targeting.drop(ALL_TARGETS, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['target_all']\n",
      "target proportion target_all\n",
      "0.0215432748012\n",
      "0.0215432748012\n"
     ]
    }
   ],
   "source": [
    "#check target percentage in population\n",
    "print(ALL_TARGETS)\n",
    "\n",
    "for target in ALL_TARGETS:\n",
    "    print(\"target proportion %s\"%target)\n",
    "    print(dataset_modelA_clean_targeting[target].mean())\n",
    "    print(dataset_modelA_clean_targeting[target].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create targeting model and store it as pickle object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start TARGETING MODELA CREATION WITH BAGGING OF XGB\n",
      "STROING MODELA TARGETING MODEL TO PICKLE\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "print('Start TARGETING MODELA CREATION WITH BAGGING OF XGB')\n",
    "\n",
    "target = 'target_all'\n",
    "\n",
    "clf = xgb.XGBClassifier(objective='binary:logistic', max_depth=6, n_estimators=50, learning_rate=0.05,\n",
    "                        max_delta_step=1, min_child_weight=25, gamma=0.1, scale_pos_weight=0.85, colsample_bytree=0.85,\n",
    "                        subsample=0.85, colsample_bylevel=1, nthread=10, seed=27)\n",
    "\n",
    "#clfbag_targeting = BaggingClassifier(clf, n_estimators=10, max_samples=0.9, max_features=0.9)\n",
    "\n",
    "train_X = X_train[feature_columns].drop(['svocmasterid'], axis=1).reset_index(drop=True)\n",
    "\n",
    "y_train = np.array(dataset_modelA_clean_targeting[target].astype(np.uint8))\n",
    "\n",
    "clf.fit(train_X, y_train)\n",
    "\n",
    "print('STROING MODELA TARGETING MODEL TO PICKLE')\n",
    "joblib.dump(clf,'model_savingpath', compress=1)\n",
    "print('DONE!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_importance(_bst, _importance_type):\n",
    "    # if it's weight, then omap stores the number of missing values\n",
    "    fmap = ''\n",
    "    if _importance_type == 'weight':\n",
    "        # do a simpler tree dump to save time\n",
    "        trees = _bst.get_dump(fmap, with_stats=False)\n",
    "\n",
    "        fmap = {}\n",
    "        for tree in trees:\n",
    "            for line in tree.split('\\n'):\n",
    "                # look for the opening square bracket\n",
    "                arr = line.split('[')\n",
    "                # if no opening bracket (leaf node), ignore this line\n",
    "                if len(arr) == 1:\n",
    "                    continue\n",
    "\n",
    "                # extract feature name from string between []\n",
    "                fid = arr[1].split(']')[0].split('<')[0]\n",
    "\n",
    "                if fid not in fmap:\n",
    "                    # if the feature hasn't been seen yet\n",
    "                    fmap[fid] = 1\n",
    "                else:\n",
    "                    fmap[fid] += 1\n",
    "\n",
    "        return fmap\n",
    "\n",
    "    else:\n",
    "        trees = _bst.get_dump(fmap, with_stats=True)\n",
    "\n",
    "        _importance_type += '='\n",
    "        fmap = {}\n",
    "        gmap = {}\n",
    "        for tree in trees:\n",
    "            for line in tree.split('\\n'):\n",
    "                # look for the opening square bracket\n",
    "                arr = line.split('[')\n",
    "                # if no opening bracket (leaf node), ignore this line\n",
    "                if len(arr) == 1:\n",
    "                    continue\n",
    "\n",
    "                # look for the closing bracket, extract only info within that bracket\n",
    "                fid = arr[1].split(']')\n",
    "\n",
    "                # extract gain or cover from string after closing bracket\n",
    "                g = float(fid[1].split(_importance_type)[1].split(',')[0])\n",
    "\n",
    "                # extract feature name from string before closing bracket\n",
    "                fid = fid[0].split('<')[0]\n",
    "\n",
    "                if fid not in fmap:\n",
    "                    # if the feature hasn't been seen yet\n",
    "                    fmap[fid] = 1\n",
    "                    gmap[fid] = g\n",
    "                else:\n",
    "                    fmap[fid] += 1\n",
    "                    gmap[fid] += g\n",
    "\n",
    "        # calculate average value (gain/cover) for each feature\n",
    "        for fid in gmap:\n",
    "            gmap[fid] = gmap[fid] / fmap[fid]\n",
    "\n",
    "        return gmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances_data = []\n",
    "features = feature_columns\n",
    "for feature_name, feature_importance in get_importance(clf.booster(), 'gain').iteritems():\n",
    "    feature_importances_data.append({\n",
    "       'feature': feature_name,\n",
    "       'importance': feature_importance\n",
    "    })\n",
    "    \n",
    "feature_importances = pd.DataFrame(feature_importances_data)\n",
    "feature_importances_sort = feature_importances.sort_values(by='importance',ascending=False)\n",
    "feature_importances_top50 = feature_importances_sort[:50][::-1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances['abs_imp'] = feature_importances['importance'].apply(lambda x: abs(x))\n",
    "feature_importances_sort = feature_importances.sort_values(by='abs_imp',ascending=False)\n",
    "feature_importances_sort['relative_imp'] = 100.0 * (feature_importances_sort['abs_imp'] / feature_importances_sort['abs_imp'].max())\n",
    "\n",
    "\n",
    "feature_importances_sort = feature_importances_sort[::-1].reset_index(drop=True)\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.title(\"Feature importances for Model\")\n",
    "plt.barh(feature_importances_sort.index, feature_importances_sort['relative_imp'],\n",
    "         color='#348ABD', align=\"center\", lw='3', edgecolor='#348ABD', alpha=0.6)\n",
    "plt.yticks(feature_importances_sort.index, feature_importances_sort['feature'], fontsize=12,)\n",
    "plt.ylim([-1, feature_importances_sort.index.max()+1])\n",
    "plt.xlim([0, feature_importances_sort['relative_imp'].max()*1.1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

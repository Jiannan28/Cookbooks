{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,average_precision_score,roc_curve,auc,precision_recall_curve\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import xgboost as xgb\n",
    "from scipy import interp\n",
    "\n",
    "import random\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from plotly import tools\n",
    "import plotly.plotly as py\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode()\n",
    "\n",
    "import colorlover as cl\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_importance(_bst, _importance_type):\n",
    "    # if it's weight, then omap stores the number of missing values\n",
    "    fmap = ''\n",
    "    if _importance_type == 'weight':\n",
    "        # do a simpler tree dump to save time\n",
    "        trees = _bst.get_dump(fmap, with_stats=False)\n",
    "\n",
    "        fmap = {}\n",
    "        for tree in trees:\n",
    "            for line in tree.split('\\n'):\n",
    "                # look for the opening square bracket\n",
    "                arr = line.split('[')\n",
    "                # if no opening bracket (leaf node), ignore this line\n",
    "                if len(arr) == 1:\n",
    "                    continue\n",
    "\n",
    "                # extract feature name from string between []\n",
    "                fid = arr[1].split(']')[0].split('<')[0]\n",
    "\n",
    "                if fid not in fmap:\n",
    "                    # if the feature hasn't been seen yet\n",
    "                    fmap[fid] = 1\n",
    "                else:\n",
    "                    fmap[fid] += 1\n",
    "\n",
    "        return fmap\n",
    "\n",
    "    else:\n",
    "        trees = _bst.get_dump(fmap, with_stats=True)\n",
    "\n",
    "        _importance_type += '='\n",
    "        fmap = {}\n",
    "        gmap = {}\n",
    "        for tree in trees:\n",
    "            for line in tree.split('\\n'):\n",
    "                # look for the opening square bracket\n",
    "                arr = line.split('[')\n",
    "                # if no opening bracket (leaf node), ignore this line\n",
    "                if len(arr) == 1:\n",
    "                    continue\n",
    "\n",
    "                # look for the closing bracket, extract only info within that bracket\n",
    "                fid = arr[1].split(']')\n",
    "\n",
    "                # extract gain or cover from string after closing bracket\n",
    "                g = float(fid[1].split(_importance_type)[1].split(',')[0])\n",
    "\n",
    "                # extract feature name from string before closing bracket\n",
    "                fid = fid[0].split('<')[0]\n",
    "\n",
    "                if fid not in fmap:\n",
    "                    # if the feature hasn't been seen yet\n",
    "                    fmap[fid] = 1\n",
    "                    gmap[fid] = g\n",
    "                else:\n",
    "                    fmap[fid] += 1\n",
    "                    gmap[fid] += g\n",
    "\n",
    "        # calculate average value (gain/cover) for each feature\n",
    "        for fid in gmap:\n",
    "            gmap[fid] = gmap[fid] / fmap[fid]\n",
    "\n",
    "        return gmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Targeting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files and preparation before cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_modelA_clean = pd.read_csv(\"/group/jiannan/Data/dataset.csv\",sep=\"|\",na_values=[\"\\N\", \"NULL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop ModelB targets\n",
    "new_set_flg = False\n",
    "\n",
    "if new_set_flg:\n",
    "    modelB_targets = ['target_rider_all_max','target_rider_waiver_premium_max','target_rider_health_max',\n",
    "                      'target_rider_accident_max','target_rider_critical_illness_max','target_rider_term_max']\n",
    "    dataset_modelA_clean = dataset_modelA_clean.drop(modelB_targets, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_modelA_clean=dataset_modelA_clean.rename(columns = {'target_whole_life_max':'target_whole_life',\n",
    "                                 'target_endowment_max':'target_endowment',\n",
    "                                 'target_retirement_max':'target_retirement',\n",
    "                                 'target_short_term_saving_max':'target_short_term_saving',\n",
    "                                 'target_term_life_max':'target_term_life',\n",
    "                                 'target_universal_life_max':'target_universal_life',\n",
    "                                 'target_health_max':'target_health',\n",
    "                                 'target_all_max':'target_all'\n",
    "                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_TARGETS = ['target_whole_life','target_endowment','target_retirement','target_short_term_saving','target_term_life',\n",
    "               'target_universal_life','target_health','target_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targeting_features_file = '/group/jiannan/Cols/modelA_targeting_features.csv'\n",
    "feature_df_targeting = pd.read_csv(targeting_features_file,na_values=[\"\\N\",\"NULL\"])\n",
    "\n",
    "features_columns_targeting = feature_df_targeting.targeting_features.tolist()\n",
    "\n",
    "print(len(features_columns_targeting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targeting_features_file = '/group/jiannan/Cols/modelA_reco_features.csv'\n",
    "feature_df_reco = pd.read_csv(targeting_features_file,na_values=[\"\\N\",\"NULL\"])\n",
    "\n",
    "features_columns_reco = feature_df_reco.reco_features.tolist()\n",
    "\n",
    "print(len(features_columns_reco))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get data set for targeting\n",
    "dataset_modelA_clean_targeting = dataset_modelA_clean[np.concatenate([features_columns_targeting, ALL_TARGETS])].copy()\n",
    "\n",
    "# get data set for recommendation\n",
    "dataset_modelA_clean_reco = dataset_modelA_clean[np.concatenate([features_columns_reco, ALL_TARGETS])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(ALL_TARGETS)\n",
    "\n",
    "for target in ALL_TARGETS:\n",
    "    print(\"target proportion %s\"%target)\n",
    "    print(dataset_modelA_clean_targeting[target].mean())\n",
    "    print(dataset_modelA_clean_targeting[target].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start CV for xgb targeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_cross_validation(_df, _classifier, _features_columns):\n",
    "    # cross validation type can be changed here\n",
    "    ss = sk.cross_validation.ShuffleSplit(len(_df.svocmasterid.unique()), n_iter=10, test_size=.3, random_state=0)\n",
    "    target='target_all'\n",
    "    prob_of = 'prob_of_all'\n",
    "    \n",
    "    results_cv_targeting = pd.DataFrame([], columns=['svocmasterid', target, 'fold', prob_of])\n",
    "\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    mean_lift = []\n",
    "    mean_tp = 0.0\n",
    "    mean_fp = range(0, 101)\n",
    "\n",
    "    nb_calls_cv = pd.DataFrame([],columns=['nb_contacts', 'total_population', 'total_X_sellers', 'nb_Xsellers', 'Xsell_rate', \n",
    "                                           'Percentage_of_Xseller_found', 'Percentage_of_Population', 'Lift'])\n",
    "    feature_importances = pd.DataFrame([], columns=['feature', 'importance', 'fold'])\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 12))\n",
    "    fig.subplots_adjust(bottom=-0.5, left=-0.5, top=0.5, right=1.5)\n",
    "\n",
    "\n",
    "    print ('modeling started')\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(ss):\n",
    "\n",
    "        customer_id = _df.svocmasterid.unique().copy()\n",
    "        shuffled_customer_id = np.array(sorted(customer_id, key=lambda k: random.random()))\n",
    "        train_customer_id = shuffled_customer_id[train_index]\n",
    "        valid_customer_id = shuffled_customer_id[valid_index]\n",
    "\n",
    "        train = _df.loc[ _df.svocmasterid.isin(train_customer_id), np.concatenate([_features_columns, [target]],\n",
    "                        axis=0)].copy().reset_index(drop=True)\n",
    "        valid = _df.loc[_df.svocmasterid.isin(valid_customer_id), np.concatenate([_features_columns, [target]],\n",
    "                        axis=0)].copy().reset_index(drop=True)\n",
    "\n",
    "        temp = valid[['svocmasterid', target]].copy()\n",
    "        temp['fold'] = i\n",
    "\n",
    "        # modeling#\n",
    "        train_X = train.drop(['svocmasterid', target], axis=1)\n",
    "        valid_X = valid.drop(['svocmasterid', target], axis=1)\n",
    "\n",
    "        train_Y = np.array(train[target].astype(np.uint8))\n",
    "        valid_Y = np.array(valid[target].astype(np.uint8))\n",
    "\n",
    "        probas_ = _classifier.fit(train_X, train_Y,eval_metric='auc', eval_set=[(valid_X, valid_Y)],early_stopping_rounds=40).predict_proba(valid_X) \n",
    "        probabilities = pd.DataFrame(data=probas_[:, 1], index=valid_X.index, columns=[prob_of])\n",
    "\n",
    "        temp = temp.join(probabilities, how='left')\n",
    "        results_cv_targeting = results_cv_targeting.append(temp)\n",
    "\n",
    "        ###############################################################################\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = sk.metrics.roc_curve(valid_Y, probas_[:, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = sk.metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(fpr, tpr, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        ###############################################################################\n",
    "        # compute lift at 10%#\n",
    "        sorted_proba = np.array(list(reversed(np.argsort(probas_[:, 1]))))\n",
    "        X_test = valid_X\n",
    "        y_test = valid_Y\n",
    "        centile = X_test.shape[0] / 100\n",
    "        positives = sum(y_test)\n",
    "        lift = [0]\n",
    "        for q in xrange(1, 101):\n",
    "            if q == 100:\n",
    "                tp = sum(np.array(y_test)[sorted_proba[(q - 1) * X_test.shape[0] / 100:X_test.shape[0]]])\n",
    "            else:\n",
    "                tp = sum(\n",
    "                    np.array(y_test)[sorted_proba[(q - 1) * X_test.shape[0] / 100:q * X_test.shape[0] / 100]])\n",
    "            lift.append(lift[q - 1] + 100 * tp / float(positives))\n",
    "        quantiles = range(0, 101)\n",
    "        mean_tp += interp(mean_fp, mean_fp, lift)\n",
    "        mean_tp[0] = 0.0\n",
    "        mean_lift.append(lift[10] / 10.)\n",
    "\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(quantiles, lift, label='Lift fold %d at 10 = %0.2f' % (i, lift[10] / 10.))\n",
    "        print ('shuffle: %i, AUC: %f, lift at 10 percent: %f' % (i, roc_auc, lift[10] / 10.))\n",
    "        \n",
    "        ###############################################################################\n",
    "        # Calculate nb calls to make\n",
    "        nb_calls = temp[['target_all','prob_of_all','fold']].copy()\n",
    "        nb_calls = nb_calls.sort_values(by='prob_of_all', ascending=False).reset_index(drop=True)\n",
    "        nb_calls['cum_Xsellers'] = np.cumsum(nb_calls.target_all)\n",
    "        nb_calls = nb_calls.reset_index(drop=False)\n",
    "        nb_calls = nb_calls.rename(columns={'index':'rank'})\n",
    "        nb_calls['nb_calls_100'] = nb_calls.loc[nb_calls.cum_Xsellers==100,'rank'].min()\n",
    "        nb_calls['nb_calls_200'] = nb_calls.loc[nb_calls.cum_Xsellers==200,'rank'].min()\n",
    "        nb_calls['nb_calls_500'] = nb_calls.loc[nb_calls.cum_Xsellers==500,'rank'].min()\n",
    "        nb_calls['nb_calls_1000'] = nb_calls.loc[nb_calls.cum_Xsellers==1000,'rank'].min()\n",
    "        nb_calls['nb_calls_2000'] = nb_calls.loc[nb_calls.cum_Xsellers==2000,'rank'].min()\n",
    "        nb_calls['nb_calls_3000'] = nb_calls.loc[nb_calls.cum_Xsellers==3000,'rank'].min()\n",
    "        nb_calls['nb_calls_all'] = nb_calls.loc[nb_calls.cum_Xsellers==nb_calls.cum_Xsellers.max(),'rank'].min()\n",
    "        nb_calls = nb_calls[['nb_calls_100','nb_calls_200', 'nb_calls_500','nb_calls_1000', 'nb_calls_2000','nb_calls_3000','nb_calls_all']].min()\n",
    "        nb_calls = pd.DataFrame(nb_calls,columns=['nb_contacts'])\n",
    "        nb_calls['total_population'] = temp.shape[0]\n",
    "        nb_calls['total_X_sellers'] = temp.target_all.sum()\n",
    "        nb_calls['nb_Xsellers']=[100,200,500,1000,2000,3000, temp.target_all.sum()]\n",
    "        nb_calls['Xsell_rate'] = nb_calls.nb_Xsellers/nb_calls.nb_contacts\n",
    "        nb_calls['Percentage_of_Xseller_found'] = nb_calls.nb_Xsellers/nb_calls.total_X_sellers\n",
    "        nb_calls['Percentage_of_Population'] = nb_calls.nb_contacts/nb_calls.total_population\n",
    "        nb_calls['Lift'] = nb_calls.Percentage_of_Xseller_found/nb_calls.Percentage_of_Population\n",
    "\n",
    "        nb_calls_cv = nb_calls_cv.append(nb_calls)\n",
    "        \n",
    "        ###############################################################################\n",
    "        feature_importances_data = []\n",
    "        features = train_X.columns\n",
    "        for feature_name, feature_importance in get_importance(_classifier.booster(), 'gain').iteritems():\n",
    "            feature_importances_data.append({\n",
    "                'feature': feature_name,\n",
    "                'importance': feature_importance\n",
    "            })\n",
    "\n",
    "        temp = pd.DataFrame(feature_importances_data)\n",
    "        temp['fold'] = i\n",
    "        feature_importances = feature_importances.append(temp)\n",
    "        \n",
    "    \n",
    "    nb_calls_cv = nb_calls_cv.reset_index().groupby('index').mean().sort_values(by='nb_Xsellers')\n",
    "    results_cv_targeting = results_cv_targeting.reset_index(drop=True)\n",
    "    \n",
    "    feature_importances = feature_importances.groupby('feature')['importance'].agg([np.mean, np.std])\n",
    "    feature_importances = feature_importances.sort_values(by='mean')\n",
    "    feature_importances = feature_importances.reset_index()\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    mean_tpr /= len(ss)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = sk.metrics.auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6))\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    mean_tp /= len(ss)\n",
    "    mean_tp[-1] = 100.0\n",
    "    mean_lift10 = np.mean(mean_lift)\n",
    "    print('Mean AUC: %f, Mean lift at 10 percent: %f' % (mean_auc, mean_lift10))\n",
    "    plt.plot(mean_fp, mean_tp, 'k--', label='Mean lift at 10 = %0.2f' % mean_lift10, lw=2)\n",
    "\n",
    "    plt.plot([0, 100], [0, 100], 'k--', color=(0.6, 0.6, 0.6))\n",
    "    plt.xlim([-5, 105])\n",
    "    plt.ylim([-5, 105])\n",
    "    plt.xlabel('Percentage of population')\n",
    "    plt.ylabel('Cumulative gain')\n",
    "    plt.title('Lift', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return results_cv_targeting, feature_importances, nb_calls_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "classifier = xgb.XGBClassifier(objective='binary:logistic',max_depth=6,n_estimators=420, learning_rate=0.05,max_delta_step=1,\n",
    "                        min_child_weight=25, gamma=0.1, scale_pos_weight=1, colsample_bytree=0.85, subsample=0.85,colsample_bylevel=0.85,\n",
    "                        nthread=14, seed=27)\n",
    "\n",
    "results_cv_targeting, feature_importances, nb_calls_cv = run_cross_validation(dataset_modelA_clean_targeting, classifier , features_columns_targeting) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances_sort = feature_importances.sort_values(by='mean',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances_sort.to_csv('/home/jliu/xgb_feat_cv.txt',sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances_sort['relative_imp'] = feature_importances_sort['mean']/feature_importances_sort['mean'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances_top40 = feature_importances_sort[:40][::-1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importances_top40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "plt.title(\"Top40 Most Important Feature importances for ModelA\")\n",
    "plt.barh(feature_importances_top40.index, feature_importances_top40['relative_imp'],\n",
    "         color='#348ABD', align=\"center\", lw='3', edgecolor='#348ABD', alpha=0.6)\n",
    "plt.yticks(feature_importances_top40.index, feature_importances_top40['feature'], fontsize=12,)\n",
    "plt.ylim([-1, feature_importances_top40.index.max()+1])\n",
    "plt.xlim([0, feature_importances_top40['relative_imp'].max()*1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_calls_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dataset_modelA_clean_reco.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target='target_multi'\n",
    "probas_cols = ['prob_of_endowment', 'prob_of_universal_life', 'prob_of_retirement', 'prob_of_term_life',\n",
    "               'prob_of_health', 'prob_of_short_term_saving', 'prob_of_whole_life']\n",
    "target_cols = [u'target_endowment', u'target_universal_life', u'target_retirement',\n",
    "               u'target_term_life', u'target_health', u'target_short_term_saving', u'target_whole_life']\n",
    "products = ['endowment','universal_life','retirement','term_life','health','short_term_saving','whole_life']\n",
    "products_name = ['endowment','universal_life','retirement','term_life','health','short_term_saving','whole_life']\n",
    "target_map = {u'endowment': 0, u'universal_life': 1, u'retirement': 2,\n",
    "              u'term_life': 3, u'health': 4, u'short_term_saving': 5, u'whole_life': 6}\n",
    "inv_target_map = {0: 'endowment', 1: 'universal_life', 2: 'retirement', 3: 'term_life', 4: 'health',\n",
    "                  5: 'short_term_saving', 6: 'whole_life'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_modelA_reco = dataset_modelA_clean_reco.loc[dataset_modelA_clean.target_all==1].copy()\n",
    "dataset_modelA_reco = dataset_modelA_reco.reset_index(drop=True)\n",
    "print('before transformation', dataset_modelA_reco.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Start to transform the matrix to do multi-classifier\n",
    "\n",
    "dataset_modelA_reco_Y = pd.DataFrame([],columns=['svocmasterid','target_multi'])\n",
    "\n",
    "#add multi-products as targets, if one customer buys multi products together, as many as rows will be created\n",
    "for code in products:\n",
    "    temp = dataset_modelA_reco.loc[dataset_modelA_reco['target_%s' %(code)]==1, ['svocmasterid']].reset_index(drop=True)\n",
    "    temp['target_multi'] = code\n",
    "    dataset_modelA_reco_Y = dataset_modelA_reco_Y.append(temp,ignore_index=True)\n",
    "\n",
    "dataset_modelA_reco = dataset_modelA_reco[[col for col in dataset_modelA_reco.columns.values if not col.startswith('target')]].merge(\n",
    "                    dataset_modelA_reco_Y, on='svocmasterid' , how='left')\n",
    "\n",
    "dataset_modelA_reco['target_multi'] = dataset_modelA_reco['target_multi'].map(str).map(target_map)\n",
    "\n",
    "print('after transformation', dataset_modelA_reco.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.unique(dataset_modelA_reco.target_multi))\n",
    "print(dataset_modelA_reco.target_multi.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What has been indeed bought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products_bought = dataset_modelA_reco.target_multi.value_counts().reset_index()\n",
    "products_bought.columns = ['product', 'nb_buy']\n",
    "products_bought['product'] = products_bought['product'].apply(lambda x : inv_target_map[x] )\n",
    "products_bought['p_buy']=products_bought.nb_buy/products_bought.nb_buy.sum()*100\n",
    "products_bought = products_bought.sort_values(by='nb_buy', ascending=True)\n",
    "products_bought['color'] = ['rgb(140,86,75)', 'rgb(31,119,180)', 'rgb(255,127,14)','rgb(214,39,40)','rgb(255,105,180)', 'rgb(44,160,44)', 'rgb(148,103,189)']\n",
    "products_bought = products_bought.sort_values(by='nb_buy', ascending=False)\n",
    "\n",
    "data = [go.Bar(\n",
    "        x=products_bought['product'].astype(str).values,\n",
    "        y=products_bought['nb_buy'].values,\n",
    "        marker=dict(color=products_bought['color'].values))\n",
    "       ]\n",
    "\n",
    "layout = dict(\n",
    "    title='Products bought by customer',\n",
    "    height=600,\n",
    "    xaxis=dict(autotick=False, ticks='outside', tick0=0, dtick=1, ticklen=8, tickwidth=1),\n",
    ")\n",
    "\n",
    "annotations = []\n",
    "\n",
    "x_data = products_bought['product'].values\n",
    "y_data = products_bought['p_buy'].values\n",
    "y_pos = products_bought['nb_buy'].values\n",
    "\n",
    "# Adding labels\n",
    "for xd, yd, ypos in zip(x_data, y_data, y_pos):\n",
    "    annotations.append(dict(xref='x1', yref='y1', x=xd, y=ypos,\n",
    "                            text=str('%.2f') %yd,\n",
    "                            font=dict(size=16, color='rgb(150,54,3)'),\n",
    "                            xanchor='center', yanchor='bottom',\n",
    "                            showarrow=False,))\n",
    "\n",
    "layout['annotations'] = annotations\n",
    "\n",
    "\n",
    "fig=go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = pd.DataFrame(dataset_modelA_reco.target_multi.value_counts(dropna=False)).reset_index()\n",
    "weights.columns = ['target_multi', 'count']\n",
    "weights['weight'] = [1, 15, 15, 15, 10, 10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_modelA_reco = dataset_modelA_reco.merge(weights[['target_multi', 'weight']], on ='target_multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start CV for recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_reco_cv(_df, _classifier , _features_columns, _target_cols, _probas_cols):\n",
    "    \n",
    "    results_cv_multi = pd.DataFrame([],columns=np.concatenate([['svocmasterid','target_multi','fold'],probas_cols]))\n",
    "    ss = sk.cross_validation.ShuffleSplit(len(_df.svocmasterid.unique()), n_iter=10, test_size=.25, random_state=0)\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(ss):\n",
    "\n",
    "        print ('shuffle %i'%(i))\n",
    "\n",
    "        customer_id = _df.svocmasterid.unique().copy()\n",
    "        shuffled_customer_id = np.array(sorted(customer_id, key = lambda k: random.random()))\n",
    "        train_customer_id = shuffled_customer_id[train_index]\n",
    "        valid_customer_id = shuffled_customer_id[valid_index]\n",
    "\n",
    "        train = _df.loc[_df.svocmasterid.isin(train_customer_id),np.concatenate([_features_columns,['target_multi', 'weight']],axis=0)].copy().reset_index(drop=True)\n",
    "        valid = _df.loc[_df.svocmasterid.isin(valid_customer_id),np.concatenate([_features_columns,['target_multi', 'weight']],axis=0)].copy().reset_index(drop=True)\n",
    "\n",
    "        temp = valid[['svocmasterid', 'target_multi']].copy()\n",
    "        temp['fold'] = i\n",
    "\n",
    "        #modeling#\n",
    "        train_X = train.drop(['svocmasterid', 'target_multi', 'weight'], axis=1)\n",
    "        valid_X = valid.drop(['svocmasterid', 'target_multi', 'weight'], axis=1)\n",
    "\n",
    "        train_Y = np.array(train['target_multi'].astype(np.uint8))\n",
    "        valid_Y = np.array(valid['target_multi'].astype(np.uint8))\n",
    "\n",
    "        _classifier.fit(train_X, train_Y, sample_weight = train['weight'])\n",
    "\n",
    "        _probas = _classifier.predict_proba(valid_X)\n",
    "        probabilities = pd.DataFrame(data=_probas, index=valid_X.index, columns=_probas_cols)\n",
    "\n",
    "        _preds = _classifier.predict(valid_X)\n",
    "        predictions = pd.DataFrame(data=_preds, index=valid_X.index, columns=['prediction'])\n",
    "\n",
    "        temp = temp.join(probabilities, how='left')\n",
    "        temp = temp.join(predictions, how='left')\n",
    "        results_cv_multi = results_cv_multi.append(temp)\n",
    "\n",
    "    results_cv_multi = results_cv_multi.reset_index(drop=True)\n",
    "    binary_targets = pd.get_dummies(results_cv_multi.target_multi)\n",
    "    binary_targets.columns = _target_cols\n",
    "\n",
    "    results_cv_multi = results_cv_multi.join(binary_targets,how='left')\n",
    "    print ('Done!')\n",
    "    return results_cv_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in dataset_modelA_reco.columns.values:\n",
    "    if dataset_modelA_reco[col].isnull().values.any():\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(objective = \"multi:softprob\", max_depth=7, n_estimators=200, learning_rate=0.1,\n",
    "                        max_delta_step=0,min_child_weight=20, gamma=1, scale_pos_weight=1, colsample_bytree=0.85,\n",
    "                        subsample=0.85,colsample_bylevel=0.85, nthread=14, seed=27)\n",
    "\n",
    "results_cv_multi= run_reco_cv(dataset_modelA_reco, clf, features_columns_reco, target_cols, probas_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### To check the performance results\n",
    "results_cv_multi_2 = results_cv_multi.copy()\n",
    "results_cv_multi_2 = results_cv_multi_2.groupby(['svocmasterid'], as_index=False).max()\n",
    "\n",
    "def results_recommendation_cv(_results_test, _probas_cols, _target_cols):\n",
    "    dict_reco = dict({'prob_of_endowment': 'endowment',\n",
    "                      'prob_of_universal_life': 'universal_life',\n",
    "                      'prob_of_retirement': 'retirement',\n",
    "                      'prob_of_term_life': 'term_life',\n",
    "                      'prob_of_health': 'health',\n",
    "                      'prob_of_short_term_saving': 'short_term_saving',\n",
    "                      'prob_of_whole_life': 'whole_life'\n",
    "                      })\n",
    "\n",
    "    results_reco = _results_test[np.concatenate([['svocmasterid'], _probas_cols])].copy()\n",
    "    results_reco = results_reco.set_index(['svocmasterid'])\n",
    "\n",
    "    arank = results_reco.apply(np.argsort, axis=1)\n",
    "    ranked_cols = results_reco.columns.to_series()[arank.values[:, ::-1][:, :len(_probas_cols)]]\n",
    "    results_reco2 = pd.DataFrame(ranked_cols, index=results_reco.index)\n",
    "    results_reco2 = results_reco2.reset_index(drop=False)\n",
    "    reco_cols = ['recommendation%i' % (i + 1) for i in range(len(_probas_cols))]\n",
    "    results_reco2.columns = np.concatenate([['svocmasterid'], reco_cols])\n",
    "\n",
    "    results_test2 = _results_test.merge(results_reco2, on=['svocmasterid'], how='left')\n",
    "\n",
    "    for i in range(1, len(_probas_cols) + 1):\n",
    "        results_test2['recommendation%i' % (i)] = results_test2['recommendation%i' % (i)].apply(\n",
    "            lambda x: dict_reco[x])\n",
    "\n",
    "    results_test2['nb_product_bought'] = results_test2[_target_cols].sum(axis=1)\n",
    "    return results_test2\n",
    "\n",
    "results_cv_multi_2 = results_recommendation_cv(results_cv_multi_2, probas_cols, target_cols)\n",
    "\n",
    "\n",
    "def results_product_bought(_df, _products_code, _products_name):\n",
    "\n",
    "    _products_name2 = [str.replace(prod, ' ', '') for prod in _products_name]\n",
    "\n",
    "    _results_test = _df.copy()\n",
    "\n",
    "    for code, code_name in zip(_products_code, _products_name2):\n",
    "        _results_test['product_%s' % (code)] = ''\n",
    "        _results_test.loc[_results_test['target_%s' % (code)] == 1, 'product_%s' % (code)] = code_name + ' '\n",
    "\n",
    "    _results_test['product_bought'] = _results_test[['product_' + prod for prod in _products_code]].sum(axis=1)\n",
    "    _results_test.loc[_results_test.nb_product_bought == 0, 'product_bought'] = 'Nothing'\n",
    "    _results_test.loc[_results_test.nb_product_bought == 1, 'product_bought'] = _results_test.product_bought.apply(\n",
    "        lambda x: str.replace(x, ' ', ''))\n",
    "    _results_test = _results_test.drop(['product_' + prod for prod in _products_code], axis=1)\n",
    "    return _results_test\n",
    "\n",
    "def results_reco_efficiency(_results_test, _print):\n",
    "\n",
    "    effective_buy_product = _results_test.copy()\n",
    "\n",
    "    effective_buy_product['correct_reco1'] = effective_buy_product.apply(lambda x: x['recommendation1'] in x['product_bought'], axis=1)\n",
    "    effective_buy_product['correct_reco2'] = effective_buy_product.apply(lambda x: (x['recommendation1'] in x['product_bought']) |\n",
    "                                                                         (x['recommendation2'] in x['product_bought']), axis=1)\n",
    "    effective_buy_product['correct_reco3'] = effective_buy_product.apply(lambda x: (x['recommendation1'] in x['product_bought']) | \n",
    "                                                                         (x['recommendation2'] in x['product_bought']) | \n",
    "                                                                         (x['recommendation3'] in x['product_bought']), axis=1)\n",
    "    if _print == 1:\n",
    "        print('out of %i customers, %i (%.2f perc.) bought the product recommended'\n",
    "              % (effective_buy_product.shape[0], effective_buy_product.correct_reco1.sum(),\n",
    "                 effective_buy_product.correct_reco1.mean() * 100))\n",
    "        print('out of %i customers, %i (%.2f perc.) bought the product recommended'\n",
    "              % (effective_buy_product.shape[0], effective_buy_product.correct_reco2.sum(),\n",
    "                 effective_buy_product.correct_reco2.mean() * 100))\n",
    "        print('out of %i customers, %i (%.2f perc.) bought the product recommended'\n",
    "              % (effective_buy_product.shape[0], effective_buy_product.correct_reco3.sum(),\n",
    "                 effective_buy_product.correct_reco3.mean() * 100))\n",
    "\n",
    "    return effective_buy_product\n",
    "\n",
    "\n",
    "results_cv_multi_3 = results_product_bought(results_cv_multi_2, products, products_name)\n",
    "\n",
    "effective_buy_cv_multi = pd.DataFrame(['correct_reco1','correct_reco2','correct_reco3'], columns=['nb_product_recommend'])\n",
    "\n",
    "effective_buy_cv_multi = results_reco_efficiency(results_cv_multi_3, 1)\n",
    "effective_buy_cv_multi = effective_buy_cv_multi[['correct_reco1','correct_reco2','correct_reco3']].mean()*100\n",
    "effective_buy_cv_multi = effective_buy_cv_multi.reset_index()\n",
    "effective_buy_cv_multi = effective_buy_cv_multi.rename(columns={'index':'nb_product_recommend', 0 : 'correct_reco_mean'})\n",
    "\n",
    "effective_buy_cv_multi[['nb_product_recommend', 'correct_reco_mean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def figure_diversity(_df_diversity, _model_title):\n",
    "    diversity_stat = _df_diversity.copy()\n",
    "\n",
    "    fig = {\n",
    "        \"data\": [  \n",
    "            {\"values\": diversity_stat['reco1'].values,\n",
    "             \"labels\": diversity_stat['product'].values,\n",
    "             \"name\": 'reco1',\n",
    "             \"hole\": .4,\n",
    "             \"type\": \"pie\",\n",
    "             \"rotation\":90,\n",
    "             \"textinfo\":\"percent+value+label\",\n",
    "             \"textposition\":\"inside\",\n",
    "             \"marker\": {'colors': diversity_stat['color'].values},\n",
    "             \"domain\": {'x': [0, .45],\n",
    "                        'y': [.55, 1]}\n",
    "            },\n",
    "            {\"values\": diversity_stat['reco2'].values,\n",
    "             \"labels\": diversity_stat['product'].values,\n",
    "             \"name\": 'reco2',\n",
    "             \"hole\": .4,\n",
    "             \"type\": \"pie\",\n",
    "             \"rotation\":90,\n",
    "             \"textinfo\":\"percent+value+label\",\n",
    "             \"textposition\":\"inside\",\n",
    "             \"marker\": {'colors': diversity_stat['color'].values},\n",
    "             \"domain\": {'x': [.55, 1],\n",
    "                        'y': [.55, 1]}\n",
    "            },\n",
    "            {\"values\": diversity_stat['reco3'].values,\n",
    "             \"labels\": diversity_stat['product'].values,\n",
    "             \"name\": 'reco3',\n",
    "             \"hole\": .4,\n",
    "             \"type\": \"pie\",\n",
    "             \"rotation\":90,\n",
    "             \"textinfo\":\"percent+value+label\",\n",
    "             \"textposition\":\"inside\",\n",
    "             \"marker\": {'colors': diversity_stat['color'].values},\n",
    "             \"domain\": {'x': [0, .45],\n",
    "                        'y': [0, .45]}\n",
    "            },\n",
    "            {\"values\": diversity_stat['sum'].values,\n",
    "             \"labels\": diversity_stat['product'].values,\n",
    "             \"name\": 'sum',\n",
    "             \"hole\": .4,\n",
    "             \"type\": \"pie\",\n",
    "             \"rotation\":90,\n",
    "             \"textinfo\":\"percent+value+label\",\n",
    "             \"textposition\":\"inside\",\n",
    "             \"marker\": {'colors': diversity_stat['color'].values},\n",
    "             \"domain\": {'x': [.55, 1],\n",
    "                        'y': [0, .45]}\n",
    "            }\n",
    "        ],\n",
    "        \"layout\": {\n",
    "            \"title\": _model_title,\n",
    "            \"width\":950,\n",
    "            \"height\":1000,\n",
    "            \"margin\":go.Margin(\n",
    "                l=0,\n",
    "                r=0,\n",
    "                b=100,\n",
    "                t=100,\n",
    "                pad=4\n",
    "            ),\n",
    "            \"showlegend\":True,\n",
    "            \"legend\":dict(\n",
    "                x=0.43,\n",
    "                y=0.5\n",
    "            ),\n",
    "            \"annotations\": [\n",
    "            {\n",
    "                \"font\": {\n",
    "                    \"size\": 20\n",
    "                },\n",
    "                \"showarrow\": False,\n",
    "                \"text\": \"1st Reco\",\n",
    "                \"x\": 0.17,\n",
    "                \"y\": 0.79\n",
    "            },\n",
    "            {\n",
    "                \"font\": {\n",
    "                    \"size\": 20\n",
    "                },\n",
    "                \"showarrow\": False,\n",
    "                \"text\": \"2nd Reco\",\n",
    "                \"x\": 0.83,\n",
    "                \"y\": 0.79\n",
    "            },\n",
    "            {\n",
    "                \"font\": {\n",
    "                    \"size\": 20\n",
    "                },\n",
    "                \"showarrow\": False,\n",
    "                \"text\": \"3rd Reco\",\n",
    "                \"x\": 0.17,\n",
    "                \"y\": 0.21\n",
    "            },\n",
    "            {\n",
    "                \"font\": {\n",
    "                    \"size\": 20\n",
    "                },\n",
    "                \"showarrow\": False,\n",
    "                \"text\": \"Sum Reco\",\n",
    "                \"x\": 0.84,\n",
    "                \"y\": 0.21\n",
    "            }\n",
    "        ]\n",
    "        }\n",
    "    }\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_color=dict({\n",
    "        'universal_life' : 'rgb(31,119,180)',\n",
    "        'retirement' :'rgb(255,127,14)',\n",
    "        'short_term_saving' : 'rgb(44,160,44)',\n",
    "        'term_life' : 'rgb(214,39,40)',\n",
    "        'whole_life' : 'rgb(148,103,189)',\n",
    "        'endowment' : 'rgb(140,86,75)',\n",
    "        'health': 'rgb(255,105,180)'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommend_info = pd.DataFrame(dict_color.keys())\n",
    "recommend_info.columns = ['product']\n",
    "recommend_info['color'] = recommend_info['product'].apply(lambda x : dict_color[x])\n",
    "\n",
    "reco1 = results_cv_multi_3.groupby(['recommendation1'], as_index=False).count()[['recommendation1', 'svocmasterid']]\n",
    "reco2 = results_cv_multi_3.groupby(['recommendation2'], as_index=False).count()[['recommendation2', 'svocmasterid']]\n",
    "reco3 = results_cv_multi_3.groupby(['recommendation3'], as_index=False).count()[['recommendation3', 'svocmasterid']]\n",
    "\n",
    "reco1.columns = ['product', 'reco1']\n",
    "reco2.columns = ['product', 'reco2']\n",
    "reco3.columns = ['product', 'reco3']\n",
    "\n",
    "recommend_info = recommend_info.merge(reco1, on='product', how='left')\n",
    "recommend_info = recommend_info.merge(reco2, on='product', how='left')\n",
    "recommend_info = recommend_info.merge(reco3, on='product', how='left')\n",
    "\n",
    "recommend_info = recommend_info.fillna(0)\n",
    "recommend_info['sum'] = recommend_info['reco1'] + recommend_info['reco2'] + recommend_info['reco3']\n",
    "\n",
    "recommend_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure_diversity(recommend_info, \"Recommendation Diversify\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

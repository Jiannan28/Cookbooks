{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Data Science Training</center>\n",
    "<center><b>Data preparation Template</b><br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and check basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"input_file_location\",sep=\",\",na_values=[\"\\N\", \"NULL\",\"NA\",\"\"]) \n",
    "#dtype={'col1': np.str, 'col2' : np.int64, 'f2':np.float64} we can also specify column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.info()\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check data types\n",
    "check = dataset.columns.to_series().groupby(dataset.dtypes).groups\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check if some rows have null value in target columns\n",
    "print(dataset[[col for col in dataset.columns.values if col.startswith('target_column')]].isnull().any(axis=1).sum())\n",
    "#print(dataset[[col for col in dataset.columns.values if (col == 'target_column')]].isnull().any(axis=1).sum())\n",
    "\n",
    "#drop rows not having target values\n",
    "dataset=dataset[~ dataset[[col for col in dataset.columns.values if col.startswith('target_column')]].isnull().any(axis=1)].copy()\n",
    "print 'Base data has %i rows and %i columns' % (dataset.shape[0], dataset.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data (DateTime, Outliers, Numerci types ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Date/Time fields generally need to be first converted to pandas datetime format. Some ideas about datetime related features\n",
    "- Vintage\n",
    "- Time difference\n",
    "- Time flag</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#process dates features#\n",
    "date_features = {'datetime_f1' : '%m/%d/%Y %H:%M',\n",
    "                 'datetime_f1' : '%Y-%m-%d %H:%M'\n",
    "                }\n",
    "\n",
    "for feature, date_format in date_features.iteritems():\n",
    "    print '{} date format is {}'.format(feature, date_format)\n",
    "    dataset[feature] = pd.to_datetime(dataset[feature], format=date_format)\n",
    "    outliers = dataset[feature][(dataset[feature]<'1910-01-01') | (dataset[feature]>'2200-01-01')]\n",
    "    for idx in outliers.index:\n",
    "        print \"oulier at line %i\" % idx\n",
    "        loc = dataset.index.get_loc(idx)\n",
    "        dataset[feature][loc] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SNAPSHOT_DATE='2016-11-30'\n",
    "        \n",
    "dataset['Time_vintage_age'] = (pd.to_datetime(SNAPSHOT_DATE) - dataset['datetime_f1']).astype('timedelta64[D]') \n",
    "#M - Month; D - Day; W - Week; h - hour; m - minute; s - second;\n",
    "dataset['Time_difference'] = (dataset['datetime_f2'] - dataset['datetime_f1']).astype('timedelta64[h]')\n",
    "dataset['Having_time_flg'] = dataset['datetime_f1'].apply(lambda x: 0 if pd.isnull(x) else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Make sure numerical cols contains only numerical values </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMERCIAL_FEATURES = [key for key in dict(dataset.dtypes) if dict(dataset.dtypes)[key] in ['float64', 'int64']]\n",
    "\n",
    "for feature in NUMERCIAL_FEATURES:\n",
    "    dataset[feature] = pd.to_numeric(dataset[feature], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Remove outliers for columns, here need understanding the meaning of different columns </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make sure some features not negative\n",
    "pos_features=['feature_names'] # Specify list of features\n",
    "for feature in pos_features:\n",
    "    dataset.ix[dataset[feature]<=0.0, feature]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make sure some features contain only some certain values\n",
    "gender_features=[feat for feat in dataset.columns if 'sex' in feat] # Here just use gender features as example\n",
    "\n",
    "for feature in gender_features:\n",
    "    print feature\n",
    "    dataset.ix[(dataset[feature]!='M') & (dataset[feature]!='F') , feature]=np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop useless and tech columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_cols = [] #Specify your lists\n",
    "useless_cols = [] #Specify your lists\n",
    "cols_to_drop = tech_cols + useless_cols\n",
    "model_dataset = dataset.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the columns has missing values\n",
    "plt.figure(figsize = (10, 6))\n",
    "na_fractions = dataset.isnull().sum()/len(dataset)\n",
    "na_fractions.plot(kind = 'barh', xlim = (0, 1))\n",
    "plt.title('Fraction of missing values by variable', fontsize=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_imput = [\n",
    "\n",
    "    ## ZERO IMPUTATION    \n",
    "    {'feature': 'missing_f1', 'impute_with': 'ZERO'}\n",
    "\n",
    "    ## Constant IMPUTATION\n",
    "    ,{'feature': 'missing_f2', 'impute_with': 'ONE'}\n",
    "\n",
    "\n",
    "    ## MEDIAN IMPUTATION\n",
    "    ,{'feature': 'missing_f3', 'impute_with': 'MEDIAN'}\n",
    "    \n",
    "    ## CUSTOMIZED MEDIAN IMPUTATION\n",
    "    , {'feature': 'missing_f4', 'impute_with': 'CUST_MEDIAN'}\n",
    "    \n",
    "    ## AVERAGE IMUTATION\n",
    "    , {'feature': 'missing_f5', 'impute_with': 'MEAN'}\n",
    "\n",
    "    ## NEW CATEGORY IMPUTATION\n",
    "    , {'feature': 'missing_f6', 'impute_with': 'CREATE_CATEGORY'}\n",
    "\n",
    "    ## MODE IMPUTATION\n",
    "    , {'feature': 'missing_f7', 'impute_with': 'MODE'}\n",
    "    \n",
    "    ## CUSTOMIZED MODE IMPUTATION\n",
    "    , {'feature': 'missing_f8', 'impute_with': 'CUST_MODE'} \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Features for which we impute missing values\n",
    "for feature in dict_imput:\n",
    "    if feature['impute_with'] == 'MEAN':\n",
    "        v = model_dataset[feature['feature']].mean()\n",
    "    elif feature['impute_with'] == 'MEDIAN':\n",
    "        v = model_dataset[feature['feature']].median()\n",
    "    elif feature['impute_with'] == 'ZERO':\n",
    "        v = 0\n",
    "    elif feature['impute_with'] == 'ONE':\n",
    "        v = 1\n",
    "    elif feature['impute_with'] == 'CREATE_CATEGORY':\n",
    "        v = 'NULL_CATEGORY'\n",
    "    elif feature['impute_with'] == 'MODE':\n",
    "        v = model_dataset[feature['feature']].value_counts(dropna=True).index[0]        \n",
    "    elif feature['impute_with'] == 'CUST_MEDIAN':\n",
    "        v = model_dataset[[feature['feature'],'svocmasterid']].drop_duplicates().reset_index(drop=True)[feature['feature']].median()\n",
    "    elif feature['impute_with'] == 'CUST_MODE':\n",
    "        v = model_dataset[[feature['feature'],'svocmasterid']].drop_duplicates().reset_index(drop=True)[feature['feature']].value_counts(dropna=True).index[0]\n",
    "\n",
    "    model_dataset[feature['feature']] = model_dataset[feature['feature']].fillna(v)\n",
    "    print 'Imputed missing values in feature %s with value %s' % (feature['feature'], unicode(str(v), 'utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Other fillna methods\n",
    "model_dataset[feature['feature']] = model_dataset[feature['feature']].fillna(method=\"pad\")\n",
    "model_dataset[feature['feature']] = model_dataset[feature['feature']].fillna(method=\"bfill\")\n",
    "model_dataset[feature['feature']] = model_dataset[feature['feature']].fillna(model_dataset[feature['feature2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check if imputing applied on all the desired columns\n",
    "for col in model_dataset.columns.values:\n",
    "    if model_dataset[col].isnull().values.any():\n",
    "        print col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other feature creation practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Creating Bins </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bin values to create new features, here use age as example\n",
    "labels = [\"child\", \"adult\", \"elder\"]\n",
    "model_dataset['Age_group'] = pd.cut(model_dataset['Age'], bins=[0, 15, 60, inf], right=False, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Ranking </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pct_rank_qcut(series, n):\n",
    "    edges = pd.Series([float(i) / n for i in range(n + 1)])\n",
    "    f = lambda x: (edges >= x).argmax()\n",
    "    return series.rank(pct=1).apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model_dataset['feat_ranking'] = pd.qcut(model_dataset['feat'], 10).codes + 1\n",
    "#model_dataset['feat_ranking'] = pd.qcut(model_dataset['feat'].rank(method='first').values, 10).codes + 1\n",
    "model_dataset['feat_ranking'] = pct_rank_qcut(model_dataset['feat'],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Scaling (log10, Standardize, Normalize...) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dataset['feat_scaling'] = model_dataset['feat'].apply(lambda x : np.log10(x) if x != 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_value = model_dataset['feat'].dropna().mean()\n",
    "max_value = model_dataset['feat'].dropna().max()\n",
    "min_value = model_dataset['feat'].dropna().min()\n",
    "std_value = model_dataset['feat'].dropna().std()\n",
    "\n",
    "model_dataset['max_min_feat'] = model_dataset['feat'].apply(lambda x: (x - mean_value ) / (max_value - min_value ))\n",
    "model_dataset['std_feat'] = model_dataset['feat'].apply(lambda x: (x - mean_value ) / std_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Columns interaction </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_features(x):\n",
    "    if (x['feat1'] == 'value1') and (x['feat2'] == 'value2'):\n",
    "        return 1\n",
    "    elif (x['feat1'] == 'value1') and (x['feat2'] == 'value3'):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dataset['feat_combine'] = model_dataset.apply(combine_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define aggregation functions and different types of category features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first(x):\n",
    "    return x.iloc[0]\n",
    "\n",
    "def concat(x):\n",
    "    return list(set(x.tolist()))\n",
    "\n",
    "def most_present(x):\n",
    "    if all(map(pd.isnull,x)):\n",
    "        return x.iloc[0]\n",
    "    else:\n",
    "        try:\n",
    "            return x.value_counts().index[0]\n",
    "        except IndexError:\n",
    "            return x.iloc[0]\n",
    "\n",
    "def change_flg(x):\n",
    "    if len(x) == 1:\n",
    "        return 0\n",
    "    elif x.iloc[0] != x.iloc[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def last(x):\n",
    "    return x.iloc[-1]\n",
    "\n",
    "def second(x):\n",
    "    if len(x) == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return x.iloc[1]\n",
    "\n",
    "        \n",
    "agg_dict = {\n",
    "            'features_to_agg':[first,change_flg,last,second,most_present,max,min,sum,np.mean,pd.Series.median,pd.Series.count,\n",
    "                       pd.Series.nunique],\n",
    "            'target_all': max\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OTHER_CATEGORIES = \"OTHER_CATEGORIES\"\n",
    "SEPARATOR = \"_\"\n",
    "\n",
    "CATEGORIES_REC_FEATURES_DICT = {\n",
    "    'insured_occupation_class_first': [1,2,3,4,5,6,7],\n",
    "    'psex_first': [0,1],\n",
    "}\n",
    "\n",
    "CATEGORIES_HIS_FEATURES_DICT = {\n",
    "    'cash_value_flg':[0,1]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy multi-label category features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_column_name(col_name, suffix):\n",
    "    return '{}{}{}'.format(col_name, SEPARATOR, suffix)\n",
    "\n",
    "for feature, values_to_dummy in CATEGORIES_HIS_FEATURES_DICT.iteritems():\n",
    "    print 'Dummy feature %s'%(feature)\n",
    "    print 'The values to dummy',values_to_dummy\n",
    "    for value in values_to_dummy:\n",
    "        new_col = create_column_name(feature,value)\n",
    "        model_dataset[new_col] = model_dataset[feature].apply(lambda x: 1 if x==value else 0)\n",
    "    new_col = create_column_name(feature, OTHER_CATEGORIES)\n",
    "    model_dataset[new_col] = model_dataset[feature].apply(lambda x: 0 if x in values_to_dummy else 1)\n",
    "    model_dataset.drop(feature, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort by date desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sort dataset by date\n",
    "model_dataset_sort = model_dataset.sort(['pdoi'],ascending=False)\n",
    "model_dataset_sort = model_dataset_sort.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time model_dataset_agg = model_dataset_sort.groupby(['svocmasterid'],as_index=False).aggregate(agg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dataset_agg.columns = ['_'.join(col).strip() for col in model_dataset_agg.columns.values]\n",
    "model_dataset_agg.rename(columns={'svocmasterid_':'svocmasterid'}, inplace=True)\n",
    "model_dataset_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy multi-class category features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feature, values_to_dummy in CATEGORIES_REC_FEATURES_DICT.iteritems():\n",
    "    print 'Dummy feature %s'%(feature)\n",
    "    print 'The values to dummy',values_to_dummy\n",
    "    for value in values_to_dummy:\n",
    "        new_col = create_column_name(feature,value)\n",
    "        model_dataset_agg[new_col] = model_dataset_agg[feature].apply(lambda x : 1 if x==value else 0)\n",
    "    new_col = create_column_name(feature,OTHER_CATEGORIES)\n",
    "    model_dataset_agg[new_col] = model_dataset_agg[feature].apply(lambda x: 0 if x in values_to_dummy else 1)\n",
    "    model_dataset_agg.drop(feature,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dataframe only contains non empty numerical columns to be prepared for next phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_modelA = model_dataset_agg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check = dataset_modelA.columns.to_series().groupby(dataset_modelA.dtypes).groups\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check if imputing applied on all the desired columns\n",
    "for col in model_dataset.columns.values:\n",
    "    if model_dataset[col].isnull().values.any():\n",
    "        print col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 sets of metadata:\n",
    "Imputing Empty Values <br>\n",
    "Aggregation methods <br>\n",
    "Categorical values <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data prepartion output for next phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_modelA.to_csv('output_file_location', sep='|', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

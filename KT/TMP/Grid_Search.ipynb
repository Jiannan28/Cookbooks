{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Data Science Training</center>\n",
    "<center><b>Grid Search and Cross-validation Template</b><br>\n",
    "Leo, 2016</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.cross_validation import train_test_split,KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,average_precision_score,roc_curve,auc,precision_recall_curve\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from scipy import interp\n",
    "\n",
    "import random\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files and preparation before cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_modelA_clean = pd.read_csv(\"input_file_pat'\",sep=\"|\", na_values=[\"\\N\", \"NULL\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_TARGETS = ['target']\n",
    "IDS = ['masking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targeting_features_file = 'feature_columns_metadata_path'\n",
    "feature_df_targeting = pd.read_csv(targeting_features_file,na_values=[\"\\N\",\"NULL\"])\n",
    "\n",
    "feature_columns = feature_df_targeting.targeting_features.tolist()\n",
    "\n",
    "print(len(feature_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get data set for targeting\n",
    "dataset_modelA_clean_targeting = dataset_modelA_clean[np.concatenate([feature_columns, ALL_TARGETS])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ALL_TARGETS)\n",
    "\n",
    "for target in ALL_TARGETS:\n",
    "    print(\"target number %s\"%target)\n",
    "    print(dataset_modelA_clean_targeting[target].sum())\n",
    "    print(\"target proportion %s\"%target)\n",
    "    print(dataset_modelA_clean_targeting[target].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset_modelA_clean_targeting,train_size=0.7)\n",
    "\n",
    "print('Train data has %i rows and %i columns'%(train.shape[0], train.shape[1]))\n",
    "print('Test data has %i rows and %i columns'%(test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COLS_TO_DROP = ALL_TARGETS + IDS  \n",
    "\n",
    "X_train = train.drop(COLS_TO_DROP,axis=1)\n",
    "X_test = test.drop(COLS_TO_DROP,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled =pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search + CrossVal for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_test = test[['masking','target']].copy()\n",
    "results_train = train[['masking','target']].copy()\n",
    "\n",
    "target = 'target'\n",
    "grid_parameters = {'penalty':['l2'], 'C':[0.5], 'class_weight':[None]} #'penalty':['l1','l2'] 'C':[1,0.1,0.01] 'class_weight':[None,'balanced']\n",
    "\n",
    "print(\"Start training with Grid Search\")\n",
    "print('Start LR training for target %s'%(target))\n",
    "\n",
    "clf_lg_base=LogisticRegression(n_jobs=1,random_state=27,verbose=0)\n",
    "clf = GridSearchCV(clf_lg_base,grid_parameters,scoring='roc_auc',cv=3,verbose=1,n_jobs=1)\n",
    "\n",
    "y_train = np.array(train[target].astype(np.uint8))\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\\n\")\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(\"\\n Grid scores on development set:\")\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std() / 2, params))\n",
    "\n",
    "y_test = np.array(test[target].astype(np.uint8))\n",
    "y_true, y_pred = y_test, clf.predict_proba(X_test_scaled)\n",
    "print(\"Scores on the evaluation dataset\")\n",
    "print(\"ROC AUC SCORE\\t:\\t\" + str(roc_auc_score(y_true, y_pred[:,1])))\n",
    "print(\"ACCURACY SCORE\\t:\\t\" + str(accuracy_score(y_true, clf.predict(X_test_scaled)))) #\n",
    "print(\"PRECISION SCORE\\t:\\t\" + str(average_precision_score(y_true, y_pred[:,1])))\n",
    "\n",
    "proba = y_pred[:,1]\n",
    "\n",
    "cols = ['proba_'+target]\n",
    "\n",
    "proba_df = pd.DataFrame(data=proba,index=test[feature_columns].index,columns=cols)\n",
    "results_test = pd.concat([results_test,proba_df],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_roc=pd.DataFrame([])\n",
    "results_lift=pd.DataFrame(range(0,101), columns=['quantiles'])\n",
    "\n",
    "# Compute ROC curve and area the curve\n",
    "fpr, tpr, thresholds = roc_curve(results_test[target], results_test['proba_'+target])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "temp = pd.DataFrame(np.vstack((fpr,tpr,thresholds))).T\n",
    "temp.columns = ['fpr','tpr','thresholds']\n",
    "temp['fpr'] = temp.fpr.apply(lambda x: np.around(x,decimals=2))\n",
    "temp = temp.groupby('fpr', as_index=False).agg({'tpr' : 'max', 'thresholds' : 'min'})\n",
    "temp = temp[['fpr','tpr','thresholds']]\n",
    "temp.loc[temp.fpr==0,'tpr']=0.0\n",
    "temp.columns = ['fpr_%s' %(target), 'tpr_%s' %(target),'thresholds_%s' %(target)]\n",
    "temp['roc_auc_%s' %(target)]=roc_auc\n",
    "results_roc=pd.concat([results_roc,temp],axis=1)\n",
    "\n",
    "# Compute Lift curve\n",
    "sorted_proba = np.array(list(reversed(np.argsort(results_test['proba_'+target].values))))\n",
    "xtestshape0=results_test[target].count().astype(int)\n",
    "y_test=results_test[target]\n",
    "centile = xtestshape0//100\n",
    "positives = sum(y_test)\n",
    "lift = [0]\n",
    "for q in xrange(1,101):\n",
    "    if q == 100:\n",
    "        tp = sum(np.array(y_test)[sorted_proba[(q-1)*centile:xtestshape0]])\n",
    "    else:\n",
    "        tp = sum(np.array(y_test)[sorted_proba[(q-1)*centile:q*centile]])\n",
    "    lift.append(lift[q-1]+100*tp/float(positives))\n",
    "quantiles = range(0,101)\n",
    "results_lift['lift_%s' %(target)]=lift\n",
    "results_lift['lift_10_%s' %(target)]=lift[10]/10.\n",
    "\n",
    "print(\"Model auc: %f, lift at 10: %f\" %(roc_auc, lift[10]/10.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances_data = []\n",
    "features = X_test.columns\n",
    "for feature_name, feature_importance in zip(features,clf.best_estimator_.coef_.ravel()):\n",
    "    feature_importances_data.append({\n",
    "            'feature': feature_name,\n",
    "            'importance': feature_importance\n",
    "        })\n",
    "\n",
    "feature_importances = pd.DataFrame(feature_importances_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances['abs_imp'] = feature_importances['importance'].apply(lambda x: abs(x))\n",
    "feature_importances_sort = feature_importances.sort_values(by='abs_imp',ascending=False)\n",
    "feature_importances_sort['relative_imp'] = 100.0 * (feature_importances_sort['abs_imp'] / feature_importances_sort['abs_imp'].max())\n",
    "feature_importances_sort = feature_importances_sort[::-1].reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.title(\"Feature importances for Model\")\n",
    "plt.barh(feature_importances_sort.index, feature_importances_sort['relative_imp'],\n",
    "         color='#348ABD', align=\"center\", lw='3', edgecolor='#348ABD', alpha=0.6)\n",
    "plt.yticks(feature_importances_sort.index, feature_importances_sort['feature'], fontsize=12,)\n",
    "plt.ylim([-1, feature_importances_sort.index.max()+1])\n",
    "plt.xlim([0, feature_importances_sort['relative_imp'].max()*1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_cross_validation(_df, _classifier, _features_columns):\n",
    "    # cross validation type can be changed here\n",
    "    ss = sk.cross_validation.ShuffleSplit(len(_df.masking.unique()), n_iter=3, test_size=.5, random_state=0)\n",
    "    target='target'\n",
    "    prob_of = 'prob_of_all'\n",
    "    \n",
    "    results_cv_targeting = pd.DataFrame([], columns=['masking', target, 'fold', prob_of])\n",
    "\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    mean_lift = []\n",
    "    mean_tp = 0.0\n",
    "    mean_fp = range(0, 101)\n",
    "\n",
    "    nb_calls_cv = pd.DataFrame([],columns=['nb_contacts', 'total_population', 'total_pos_targets', 'nb_pos_targets', 'pos_rate', \n",
    "                                           'Percentage_of_pos_targets_found', 'Percentage_of_Population', 'Lift'])\n",
    "    feature_importances = pd.DataFrame([], columns=['feature', 'importance', 'fold'])\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 12))\n",
    "    fig.subplots_adjust(bottom=-0.5, left=-0.5, top=0.5, right=1.5)\n",
    "\n",
    "\n",
    "    print ('modeling started')\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(ss):\n",
    "\n",
    "        customer_id = _df.masking.unique().copy()\n",
    "        shuffled_customer_id = np.array(sorted(customer_id, key=lambda k: random.random()))\n",
    "        train_customer_id = shuffled_customer_id[train_index]\n",
    "        valid_customer_id = shuffled_customer_id[valid_index]\n",
    "\n",
    "        train = _df.loc[ _df.masking.isin(train_customer_id), np.concatenate([_features_columns, [target]],\n",
    "                        axis=0)].copy().reset_index(drop=True)\n",
    "        valid = _df.loc[_df.masking.isin(valid_customer_id), np.concatenate([_features_columns, [target]],\n",
    "                        axis=0)].copy().reset_index(drop=True)\n",
    "\n",
    "        temp = valid[['masking', target]].copy()\n",
    "        temp['fold'] = i\n",
    "\n",
    "        # modeling#\n",
    "        train_X = train.drop(['masking', target], axis=1)\n",
    "        valid_X = valid.drop(['masking', target], axis=1)\n",
    "        \n",
    "        scaler = preprocessing.StandardScaler().fit(train_X)\n",
    "\n",
    "        train_X_scaled = scaler.transform(train_X)\n",
    "        valid_X_scaled = scaler.transform(valid_X)\n",
    "\n",
    "        train_X_scaled =pd.DataFrame(train_X_scaled, index=train_X.index, columns=train_X.columns)\n",
    "        valid_X_scaled = pd.DataFrame(valid_X_scaled, index=valid_X.index, columns=valid_X.columns)\n",
    "\n",
    "        train_Y = np.array(train[target].astype(np.uint8))\n",
    "        valid_Y = np.array(valid[target].astype(np.uint8))\n",
    "\n",
    "        probas_ = _classifier.fit(train_X_scaled, train_Y).predict_proba(valid_X_scaled)\n",
    "        probabilities = pd.DataFrame(data=probas_[:, 1], index=valid_X.index, columns=[prob_of])\n",
    "\n",
    "        temp = temp.join(probabilities, how='left')\n",
    "        results_cv_targeting = results_cv_targeting.append(temp)\n",
    "\n",
    "        ###############################################################################\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = sk.metrics.roc_curve(valid_Y, probas_[:, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = sk.metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(fpr, tpr, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        ###############################################################################\n",
    "        # compute lift at 10%#\n",
    "        sorted_proba = np.array(list(reversed(np.argsort(probas_[:, 1]))))\n",
    "        X_test = valid_X\n",
    "        y_test = valid_Y\n",
    "        centile = X_test.shape[0] / 100\n",
    "        positives = sum(y_test)\n",
    "        lift = [0]\n",
    "        for q in xrange(1, 101):\n",
    "            if q == 100:\n",
    "                tp = sum(np.array(y_test)[sorted_proba[(q - 1) * X_test.shape[0] / 100:X_test.shape[0]]])\n",
    "            else:\n",
    "                tp = sum(\n",
    "                    np.array(y_test)[sorted_proba[(q - 1) * X_test.shape[0] / 100:q * X_test.shape[0] / 100]])\n",
    "            lift.append(lift[q - 1] + 100 * tp / float(positives))\n",
    "        quantiles = range(0, 101)\n",
    "        mean_tp += interp(mean_fp, mean_fp, lift)\n",
    "        mean_tp[0] = 0.0\n",
    "        mean_lift.append(lift[10] / 10.)\n",
    "\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(quantiles, lift, label='Lift fold %d at 10 = %0.2f' % (i, lift[10] / 10.))\n",
    "        print ('shuffle: %i, AUC: %f, lift at 10 percent: %f' % (i, roc_auc, lift[10] / 10.))\n",
    "        \n",
    "        ###############################################################################\n",
    "        # Calculate nb contacts to make\n",
    "        nb_calls = temp[['target','prob_of_all','fold']].copy()\n",
    "        nb_calls = nb_calls.sort_values(by='prob_of_all', ascending=False).reset_index(drop=True)\n",
    "        nb_calls['cum_Xsellers'] = np.cumsum(nb_calls.target)\n",
    "        nb_calls = nb_calls.reset_index(drop=False)\n",
    "        nb_calls = nb_calls.rename(columns={'index':'rank'})\n",
    "        nb_calls['nb_contacts_100'] = nb_calls.loc[nb_calls.cum_Xsellers==100,'rank'].min()\n",
    "        nb_calls['nb_contacts_200'] = nb_calls.loc[nb_calls.cum_Xsellers==200,'rank'].min()\n",
    "        nb_calls['nb_contacts_500'] = nb_calls.loc[nb_calls.cum_Xsellers==500,'rank'].min()\n",
    "        nb_calls['nb_contacts_1000'] = nb_calls.loc[nb_calls.cum_Xsellers==1000,'rank'].min()\n",
    "        nb_calls['nb_contacts_2000'] = nb_calls.loc[nb_calls.cum_Xsellers==2000,'rank'].min()\n",
    "        nb_calls['nb_contacts_3000'] = nb_calls.loc[nb_calls.cum_Xsellers==3000,'rank'].min()\n",
    "        nb_calls['nb_contacts_all'] = nb_calls.loc[nb_calls.cum_Xsellers==nb_calls.cum_Xsellers.max(),'rank'].min()\n",
    "        nb_calls = nb_calls[['nb_contacts_100','nb_contacts_200', 'nb_contacts_500','nb_contacts_1000', 'nb_contacts_2000','nb_contacts_3000','nb_contacts_all']].min()\n",
    "        nb_calls = pd.DataFrame(nb_calls,columns=['nb_contacts'])\n",
    "        nb_calls['total_population'] = temp.shape[0]\n",
    "        nb_calls['total_pos_targets'] = temp.target.sum()\n",
    "        nb_calls['nb_pos_targets']=[100,200,500,1000,2000,3000, temp.target.sum()]\n",
    "        nb_calls['pos_rate'] = nb_calls.nb_pos_targets/nb_calls.nb_contacts\n",
    "        nb_calls['Percentage_of_pos_targets_found'] = nb_calls.nb_pos_targets/nb_calls.total_pos_targets\n",
    "        nb_calls['Percentage_of_Population'] = nb_calls.nb_contacts/nb_calls.total_population\n",
    "        nb_calls['Lift'] = nb_calls.Percentage_of_pos_targets_found/nb_calls.Percentage_of_Population\n",
    "\n",
    "        nb_calls_cv = nb_calls_cv.append(nb_calls)\n",
    "        \n",
    "        ###############################################################################\n",
    "        feature_importances_data = []\n",
    "        features = train_X.columns\n",
    "        for feature_name, feature_importance in zip(features,_classifier.coef_.ravel()):\n",
    "            feature_importances_data.append({\n",
    "                'feature': feature_name,\n",
    "                'importance': feature_importance\n",
    "            })\n",
    "\n",
    "        temp = pd.DataFrame(feature_importances_data)\n",
    "        temp['fold'] = i\n",
    "        feature_importances = feature_importances.append(temp)\n",
    "    \n",
    "    nb_calls_cv = nb_calls_cv.reset_index().groupby('index').mean().sort_values(by='nb_pos_targets')\n",
    "    results_cv_targeting = results_cv_targeting.reset_index(drop=True)\n",
    "    \n",
    "    feature_importances = feature_importances.groupby('feature')['importance'].agg([np.mean, np.std])\n",
    "    feature_importances = feature_importances.sort_values(by='mean')\n",
    "    feature_importances = feature_importances.reset_index()\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    mean_tpr /= len(ss)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = sk.metrics.auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6))\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    mean_tp /= len(ss)\n",
    "    mean_tp[-1] = 100.0\n",
    "    mean_lift10 = np.mean(mean_lift)\n",
    "    print('Mean AUC: %f, Mean lift at 10 percent: %f' % (mean_auc, mean_lift10))\n",
    "    plt.plot(mean_fp, mean_tp, 'k--', label='Mean lift at 10 = %0.2f' % mean_lift10, lw=2)\n",
    "\n",
    "    plt.plot([0, 100], [0, 100], 'k--', color=(0.6, 0.6, 0.6))\n",
    "    plt.xlim([-5, 105])\n",
    "    plt.ylim([-5, 105])\n",
    "    plt.xlabel('Percentage of population')\n",
    "    plt.ylabel('Cumulative gain')\n",
    "    plt.title('Lift', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return results_cv_targeting, feature_importances, nb_calls_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Logistic regression\n",
    "\n",
    "classifier = LogisticRegression(penalty='l2', C=0.5, class_weight=None, random_state=27)\n",
    "\n",
    "results_cv_targeting, feature_importances, nb_calls = run_cross_validation(dataset_modelA_clean_targeting, classifier , feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances['abs_imp'] = feature_importances['mean'].apply(lambda x: abs(x))\n",
    "feature_importances_sort = feature_importances.sort_values(by='abs_imp',ascending=False)\n",
    "feature_importances_sort['relative_imp'] = 100.0 * (feature_importances_sort['abs_imp'] / feature_importances_sort['abs_imp'].max())\n",
    "feature_importances_sort = feature_importances_sort[::-1].reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.title(\"Feature importances for Model\")\n",
    "plt.barh(feature_importances_sort.index, feature_importances_sort['relative_imp'],\n",
    "         color='#348ABD', align=\"center\", lw='3', edgecolor='#348ABD', alpha=0.6)\n",
    "plt.yticks(feature_importances_sort.index, feature_importances_sort['feature'], fontsize=12,)\n",
    "plt.ylim([-1, feature_importances_sort.index.max()+1])\n",
    "plt.xlim([0, feature_importances_sort['relative_imp'].max()*1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Grid Search + CrossVal for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_test = test[['masking','target']].copy()\n",
    "results_train = train[['masking','target']].copy()\n",
    "target = 'target'\n",
    "\n",
    "grid_parameters = {'n_estimators':[200],'min_samples_split':[5], 'min_samples_leaf':[5]} \n",
    "#n_estimators, criterion='gini',max_depth=None, min_samples_split=2, min_samples_leaf=1,max_features='auto',class_weight=None, \n",
    "\n",
    "\n",
    "print(\"Start training with Grid Search\")\n",
    " \n",
    "print('Start RF training for target %s'%(target))\n",
    "\n",
    "clf_rf_base=RandomForestClassifier(n_jobs=4,random_state=27,verbose=0)\n",
    "clf = GridSearchCV(clf_rf_base,grid_parameters,scoring='roc_auc',cv=3,verbose=1,n_jobs=4)\n",
    "\n",
    "y_train = np.array(train[target].astype(np.uint8))\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\\n\")\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(\"\\n Grid scores on development set:\")\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std() / 2, params))\n",
    "\n",
    "y_test = np.array(test[target].astype(np.uint8))\n",
    "y_true, y_pred = y_test, clf.predict_proba(X_test)\n",
    "print(\"Scores on the evaluation dataset\")\n",
    "print(\"ROC AUC SCORE\\t:\\t\" + str(roc_auc_score(y_true, y_pred[:,1])))\n",
    "print(\"ACCURACY SCORE\\t:\\t\" + str(accuracy_score(y_true, clf.predict(X_test))))\n",
    "print(\"PRECISION SCORE\\t:\\t\" + str(average_precision_score(y_true, y_pred[:,1])))\n",
    "\n",
    "##\n",
    "proba = y_pred[:,1]\n",
    "\n",
    "cols = ['proba_'+target]\n",
    "\n",
    "proba_df = pd.DataFrame(data=proba,index=test[feature_columns].index,columns=cols)\n",
    "results_test = pd.concat([results_test,proba_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_roc=pd.DataFrame([])\n",
    "results_lift=pd.DataFrame(range(0,101), columns=['quantiles'])\n",
    "\n",
    "# Compute ROC curve and area the curve\n",
    "fpr, tpr, thresholds = roc_curve(results_test[target], results_test['proba_'+target])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "temp = pd.DataFrame(np.vstack((fpr,tpr,thresholds))).T\n",
    "temp.columns = ['fpr','tpr','thresholds']\n",
    "temp['fpr'] = temp.fpr.apply(lambda x: np.around(x,decimals=2))\n",
    "temp = temp.groupby('fpr', as_index=False).agg({'tpr' : 'max', 'thresholds' : 'min'})\n",
    "temp = temp[['fpr','tpr','thresholds']]\n",
    "temp.loc[temp.fpr==0,'tpr']=0.0\n",
    "temp.columns = ['fpr_%s' %(target), 'tpr_%s' %(target),'thresholds_%s' %(target)]\n",
    "temp['roc_auc_%s' %(target)]=roc_auc\n",
    "results_roc=pd.concat([results_roc,temp],axis=1)\n",
    "\n",
    "# Compute Lift curve\n",
    "sorted_proba = np.array(list(reversed(np.argsort(results_test['proba_'+target].values))))\n",
    "xtestshape0=results_test[target].count().astype(int)\n",
    "y_test=results_test[target]\n",
    "centile = xtestshape0//100\n",
    "positives = sum(y_test)\n",
    "lift = [0]\n",
    "for q in xrange(1,101):\n",
    "    if q == 100:\n",
    "        tp = sum(np.array(y_test)[sorted_proba[(q-1)*centile:xtestshape0]])\n",
    "    else:\n",
    "        tp = sum(np.array(y_test)[sorted_proba[(q-1)*centile:q*centile]])\n",
    "    lift.append(lift[q-1]+100*tp/float(positives))\n",
    "quantiles = range(0,101)\n",
    "results_lift['lift_%s' %(target)]=lift\n",
    "results_lift['lift_10_%s' %(target)]=lift[10]/10.\n",
    "\n",
    "print(\"Model auc: %f, lift at 10: %f\" %(roc_auc, lift[10]/10.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances_data = []\n",
    "features = X_test.columns\n",
    "for feature_name, feature_importance in zip(features,clf.best_estimator_.feature_importances_):\n",
    "    feature_importances_data.append({\n",
    "            'feature': feature_name,\n",
    "            'importance': feature_importance\n",
    "        })\n",
    "\n",
    "feature_importances = pd.DataFrame(feature_importances_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances['abs_imp'] = feature_importances['importance'].apply(lambda x: abs(x))\n",
    "feature_importances_sort = feature_importances.sort_values(by='abs_imp',ascending=False)\n",
    "feature_importances_sort['relative_imp'] = 100.0 * (feature_importances_sort['abs_imp'] / feature_importances_sort['abs_imp'].max())\n",
    "feature_importances_sort = feature_importances_sort[::-1].reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.title(\"Feature importances for Model\")\n",
    "plt.barh(feature_importances_sort.index, feature_importances_sort['relative_imp'],\n",
    "         color='#348ABD', align=\"center\", lw='3', edgecolor='#348ABD', alpha=0.6)\n",
    "plt.yticks(feature_importances_sort.index, feature_importances_sort['feature'], fontsize=12,)\n",
    "plt.ylim([-1, feature_importances_sort.index.max()+1])\n",
    "plt.xlim([0, feature_importances_sort['relative_imp'].max()*1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_cross_validation(_df, _classifier, _features_columns):\n",
    "    # cross validation type can be changed here\n",
    "    ss = sk.cross_validation.ShuffleSplit(len(_df.masking.unique()), n_iter=5, test_size=.3, random_state=0)\n",
    "    target='target'\n",
    "    prob_of = 'prob_of_all'\n",
    "    \n",
    "    results_cv_targeting = pd.DataFrame([], columns=['masking', target, 'fold', prob_of])\n",
    "\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    mean_lift = []\n",
    "    mean_tp = 0.0\n",
    "    mean_fp = range(0, 101)\n",
    "\n",
    "    nb_calls_cv = pd.DataFrame([],columns=['nb_contacts', 'total_population', 'total_pos_targets', 'nb_pos_targets', 'pos_rate', \n",
    "                                           'Percentage_of_pos_targets_found', 'Percentage_of_Population', 'Lift'])\n",
    "    feature_importances = pd.DataFrame([], columns=['feature', 'importance', 'fold'])\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 12))\n",
    "    fig.subplots_adjust(bottom=-0.5, left=-0.5, top=0.5, right=1.5)\n",
    "\n",
    "\n",
    "    print ('modeling started')\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(ss):\n",
    "\n",
    "        customer_id = _df.masking.unique().copy()\n",
    "        shuffled_customer_id = np.array(sorted(customer_id, key=lambda k: random.random()))\n",
    "        train_customer_id = shuffled_customer_id[train_index]\n",
    "        valid_customer_id = shuffled_customer_id[valid_index]\n",
    "\n",
    "        train = _df.loc[ _df.masking.isin(train_customer_id), np.concatenate([_features_columns, [target]],\n",
    "                        axis=0)].copy().reset_index(drop=True)\n",
    "        valid = _df.loc[_df.masking.isin(valid_customer_id), np.concatenate([_features_columns, [target]],\n",
    "                        axis=0)].copy().reset_index(drop=True)\n",
    "\n",
    "        temp = valid[['masking', target]].copy()\n",
    "        temp['fold'] = i\n",
    "\n",
    "        # modeling#\n",
    "        train_X = train.drop(['masking', target], axis=1)\n",
    "        valid_X = valid.drop(['masking', target], axis=1)\n",
    "\n",
    "        train_Y = np.array(train[target].astype(np.uint8))\n",
    "        valid_Y = np.array(valid[target].astype(np.uint8))\n",
    "\n",
    "        probas_ = _classifier.fit(train_X, train_Y).predict_proba(valid_X)\n",
    "        probabilities = pd.DataFrame(data=probas_[:, 1], index=valid_X.index, columns=[prob_of])\n",
    "\n",
    "        temp = temp.join(probabilities, how='left')\n",
    "        results_cv_targeting = results_cv_targeting.append(temp)\n",
    "\n",
    "        ###############################################################################\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = sk.metrics.roc_curve(valid_Y, probas_[:, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = sk.metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(fpr, tpr, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        ###############################################################################\n",
    "        # compute lift at 10%#\n",
    "        sorted_proba = np.array(list(reversed(np.argsort(probas_[:, 1]))))\n",
    "        X_test = valid_X\n",
    "        y_test = valid_Y\n",
    "        centile = X_test.shape[0] / 100\n",
    "        positives = sum(y_test)\n",
    "        lift = [0]\n",
    "        for q in xrange(1, 101):\n",
    "            if q == 100:\n",
    "                tp = sum(np.array(y_test)[sorted_proba[(q - 1) * X_test.shape[0] / 100:X_test.shape[0]]])\n",
    "            else:\n",
    "                tp = sum(\n",
    "                    np.array(y_test)[sorted_proba[(q - 1) * X_test.shape[0] / 100:q * X_test.shape[0] / 100]])\n",
    "            lift.append(lift[q - 1] + 100 * tp / float(positives))\n",
    "        quantiles = range(0, 101)\n",
    "        mean_tp += interp(mean_fp, mean_fp, lift)\n",
    "        mean_tp[0] = 0.0\n",
    "        mean_lift.append(lift[10] / 10.)\n",
    "\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(quantiles, lift, label='Lift fold %d at 10 = %0.2f' % (i, lift[10] / 10.))\n",
    "        print ('shuffle: %i, AUC: %f, lift at 10 percent: %f' % (i, roc_auc, lift[10] / 10.))\n",
    "        \n",
    "        ###############################################################################\n",
    "        # Calculate nb contacts to make\n",
    "        nb_calls = temp[['target','prob_of_all','fold']].copy()\n",
    "        nb_calls = nb_calls.sort_values(by='prob_of_all', ascending=False).reset_index(drop=True)\n",
    "        nb_calls['cum_Xsellers'] = np.cumsum(nb_calls.target)\n",
    "        nb_calls = nb_calls.reset_index(drop=False)\n",
    "        nb_calls = nb_calls.rename(columns={'index':'rank'})\n",
    "        nb_calls['nb_contacts_100'] = nb_calls.loc[nb_calls.cum_Xsellers==100,'rank'].min()\n",
    "        nb_calls['nb_contacts_200'] = nb_calls.loc[nb_calls.cum_Xsellers==200,'rank'].min()\n",
    "        nb_calls['nb_contacts_500'] = nb_calls.loc[nb_calls.cum_Xsellers==500,'rank'].min()\n",
    "        nb_calls['nb_contacts_1000'] = nb_calls.loc[nb_calls.cum_Xsellers==1000,'rank'].min()\n",
    "        nb_calls['nb_contacts_2000'] = nb_calls.loc[nb_calls.cum_Xsellers==2000,'rank'].min()\n",
    "        nb_calls['nb_contacts_3000'] = nb_calls.loc[nb_calls.cum_Xsellers==3000,'rank'].min()\n",
    "        nb_calls['nb_contacts_all'] = nb_calls.loc[nb_calls.cum_Xsellers==nb_calls.cum_Xsellers.max(),'rank'].min()\n",
    "        nb_calls = nb_calls[['nb_contacts_100','nb_contacts_200', 'nb_contacts_500','nb_contacts_1000', 'nb_contacts_2000','nb_contacts_3000','nb_contacts_all']].min()\n",
    "        nb_calls = pd.DataFrame(nb_calls,columns=['nb_contacts'])\n",
    "        nb_calls['total_population'] = temp.shape[0]\n",
    "        nb_calls['total_pos_targets'] = temp.target.sum()\n",
    "        nb_calls['nb_pos_targets']=[100,200,500,1000,2000,3000, temp.target.sum()]\n",
    "        nb_calls['pos_rate'] = nb_calls.nb_pos_targets/nb_calls.nb_contacts\n",
    "        nb_calls['Percentage_of_pos_targets_found'] = nb_calls.nb_pos_targets/nb_calls.total_pos_targets\n",
    "        nb_calls['Percentage_of_Population'] = nb_calls.nb_contacts/nb_calls.total_population\n",
    "        nb_calls['Lift'] = nb_calls.Percentage_of_pos_targets_found/nb_calls.Percentage_of_Population\n",
    "\n",
    "        nb_calls_cv = nb_calls_cv.append(nb_calls)\n",
    "        \n",
    "        ###############################################################################\n",
    "        feature_importances_data = []\n",
    "        features = train_X.columns\n",
    "        for feature_name, feature_importance in zip(features,_classifier.feature_importances_):\n",
    "            feature_importances_data.append({\n",
    "                'feature': feature_name,\n",
    "                'importance': feature_importance\n",
    "            })\n",
    "\n",
    "        temp = pd.DataFrame(feature_importances_data)\n",
    "        temp['fold'] = i\n",
    "        feature_importances = feature_importances.append(temp)\n",
    "    \n",
    "    nb_calls_cv = nb_calls_cv.reset_index().groupby('index').mean().sort_values(by='nb_pos_targets')\n",
    "    results_cv_targeting = results_cv_targeting.reset_index(drop=True)\n",
    "    \n",
    "    feature_importances = feature_importances.groupby('feature')['importance'].agg([np.mean, np.std])\n",
    "    feature_importances = feature_importances.sort_values(by='mean')\n",
    "    feature_importances = feature_importances.reset_index()\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    mean_tpr /= len(ss)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = sk.metrics.auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6))\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    mean_tp /= len(ss)\n",
    "    mean_tp[-1] = 100.0\n",
    "    mean_lift10 = np.mean(mean_lift)\n",
    "    print('Mean AUC: %f, Mean lift at 10 percent: %f' % (mean_auc, mean_lift10))\n",
    "    plt.plot(mean_fp, mean_tp, 'k--', label='Mean lift at 10 = %0.2f' % mean_lift10, lw=2)\n",
    "\n",
    "    plt.plot([0, 100], [0, 100], 'k--', color=(0.6, 0.6, 0.6))\n",
    "    plt.xlim([-5, 105])\n",
    "    plt.ylim([-5, 105])\n",
    "    plt.xlabel('Percentage of population')\n",
    "    plt.ylabel('Cumulative gain')\n",
    "    plt.title('Lift', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return results_cv_targeting, feature_importances, nb_calls_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "classifier = RandomForestClassifier(n_estimators=200,n_jobs=4, random_state=27,min_samples_split=5, min_samples_leaf=5)\n",
    "\n",
    "results_cv_targeting, feature_importances, nb_calls = run_cross_validation(dataset_modelA_clean_targeting,classifier,feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances['abs_imp'] = feature_importances['mean'].apply(lambda x: abs(x))\n",
    "feature_importances_sort = feature_importances.sort_values(by='abs_imp',ascending=False)\n",
    "feature_importances_sort['relative_imp'] = 100.0 * (feature_importances_sort['abs_imp'] / feature_importances_sort['abs_imp'].max())\n",
    "feature_importances_sort = feature_importances_sort[::-1].reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.title(\"Feature importances for Model\")\n",
    "plt.barh(feature_importances_sort.index, feature_importances_sort['relative_imp'],\n",
    "         color='#348ABD', align=\"center\", lw='3', edgecolor='#348ABD', alpha=0.6)\n",
    "plt.yticks(feature_importances_sort.index, feature_importances_sort['feature'], fontsize=12,)\n",
    "plt.ylim([-1, feature_importances_sort.index.max()+1])\n",
    "plt.xlim([0, feature_importances_sort['relative_imp'].max()*1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search + CrossVal for XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_test = test[['masking','target']].copy()\n",
    "results_train = train[['masking','target']].copy()\n",
    "target = 'target'\n",
    "\n",
    "grid_parameters = {'max_depth': [6],'n_estimators': [200],'learning_rate':[0.05],'max_delta_step':[1],\n",
    "                   'min_child_weight':[25],'max_delta_step':[1],'gamma':[0.1],'scale_pos_weight':[1], \n",
    "                   'colsample_bytree':[0.85],'subsample':[0.85],'colsample_bylevel':[0.85]} #'scale_pos_weight':[1,n] n neg/pos\n",
    "\n",
    "print(\"Start training with Grid Search\")\n",
    "    \n",
    "print('Start Xgboost training for target %s'%(target))\n",
    "\n",
    "clf_xgb_base= xgb.XGBClassifier(nthread=10, seed=27)\n",
    "clf = GridSearchCV(clf_xgb_base,grid_parameters,scoring='roc_auc',cv=3,verbose=1,n_jobs=4) \n",
    "\n",
    "y_train = np.array(train[target].astype(np.uint8))\n",
    "clf.fit(X_train, y_train) #eval_metric='logloss', here is for early_stop if specify\n",
    "\n",
    "print(\"Best parameters set found on development set:\\n\")\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(\"Best Scores\")\n",
    "print(clf.best_score_)\n",
    "\n",
    "print(\"\\n Grid scores on development set:\")\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std() / 2, params))\n",
    "\n",
    "y_test = np.array(test[target].astype(np.uint8))\n",
    "y_true, y_pred = y_test, clf.predict_proba(X_test)\n",
    "print(\"Scores on the evaluation dataset\")\n",
    "print(\"ROC AUC SCORE\\t:\\t\" + str(roc_auc_score(y_true, y_pred[:,1])))\n",
    "print(\"ACCURACY SCORE\\t:\\t\" + str(accuracy_score(y_true, clf.predict(X_test))))\n",
    "print(\"PRECISION SCORE\\t:\\t\" + str(average_precision_score(y_true, y_pred[:,1])))\n",
    "\n",
    "proba = y_pred[:,1]\n",
    "\n",
    "cols = ['proba_'+target]\n",
    "\n",
    "proba_df = pd.DataFrame(data=proba,index=test.index,columns=cols)\n",
    "results_test = pd.concat([results_test,proba_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_roc=pd.DataFrame([])\n",
    "results_lift=pd.DataFrame(range(0,101), columns=['quantiles'])\n",
    "\n",
    "# Compute ROC curve and area the curve\n",
    "fpr, tpr, thresholds = roc_curve(results_test[target], results_test['proba_'+target])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "temp = pd.DataFrame(np.vstack((fpr,tpr,thresholds))).T\n",
    "temp.columns = ['fpr','tpr','thresholds']\n",
    "temp['fpr'] = temp.fpr.apply(lambda x: np.around(x,decimals=2))\n",
    "temp = temp.groupby('fpr', as_index=False).agg({'tpr' : 'max', 'thresholds' : 'min'})\n",
    "temp = temp[['fpr','tpr','thresholds']]\n",
    "temp.loc[temp.fpr==0,'tpr']=0.0\n",
    "temp.columns = ['fpr_%s' %(target), 'tpr_%s' %(target),'thresholds_%s' %(target)]\n",
    "temp['roc_auc_%s' %(target)]=roc_auc\n",
    "results_roc=pd.concat([results_roc,temp],axis=1)\n",
    "\n",
    "# Compute Lift curve\n",
    "sorted_proba = np.array(list(reversed(np.argsort(results_test['proba_'+target].values))))\n",
    "xtestshape0=results_test[target].count().astype(int)\n",
    "y_test=results_test[target]\n",
    "centile = xtestshape0//100\n",
    "positives = sum(y_test)\n",
    "lift = [0]\n",
    "for q in xrange(1,101):\n",
    "    if q == 100:\n",
    "        tp = sum(np.array(y_test)[sorted_proba[(q-1)*centile:xtestshape0]])\n",
    "    else:\n",
    "        tp = sum(np.array(y_test)[sorted_proba[(q-1)*centile:q*centile]])\n",
    "    lift.append(lift[q-1]+100*tp/float(positives))\n",
    "quantiles = range(0,101)\n",
    "results_lift['lift_%s' %(target)]=lift\n",
    "results_lift['lift_10_%s' %(target)]=lift[10]/10.\n",
    "\n",
    "print(\"Model auc: %f, lift at 10: %f\" %(roc_auc, lift[10]/10.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_importance(_bst, _importance_type):\n",
    "    # if it's weight, then omap stores the number of missing values\n",
    "    fmap = ''\n",
    "    if _importance_type == 'weight':\n",
    "        # do a simpler tree dump to save time\n",
    "        trees = _bst.get_dump(fmap, with_stats=False)\n",
    "\n",
    "        fmap = {}\n",
    "        for tree in trees:\n",
    "            for line in tree.split('\\n'):\n",
    "                # look for the opening square bracket\n",
    "                arr = line.split('[')\n",
    "                # if no opening bracket (leaf node), ignore this line\n",
    "                if len(arr) == 1:\n",
    "                    continue\n",
    "\n",
    "                # extract feature name from string between []\n",
    "                fid = arr[1].split(']')[0].split('<')[0]\n",
    "\n",
    "                if fid not in fmap:\n",
    "                    # if the feature hasn't been seen yet\n",
    "                    fmap[fid] = 1\n",
    "                else:\n",
    "                    fmap[fid] += 1\n",
    "\n",
    "        return fmap\n",
    "\n",
    "    else:\n",
    "        trees = _bst.get_dump(fmap, with_stats=True)\n",
    "\n",
    "        _importance_type += '='\n",
    "        fmap = {}\n",
    "        gmap = {}\n",
    "        for tree in trees:\n",
    "            for line in tree.split('\\n'):\n",
    "                # look for the opening square bracket\n",
    "                arr = line.split('[')\n",
    "                # if no opening bracket (leaf node), ignore this line\n",
    "                if len(arr) == 1:\n",
    "                    continue\n",
    "\n",
    "                # look for the closing bracket, extract only info within that bracket\n",
    "                fid = arr[1].split(']')\n",
    "\n",
    "                # extract gain or cover from string after closing bracket\n",
    "                g = float(fid[1].split(_importance_type)[1].split(',')[0])\n",
    "\n",
    "                # extract feature name from string before closing bracket\n",
    "                fid = fid[0].split('<')[0]\n",
    "\n",
    "                if fid not in fmap:\n",
    "                    # if the feature hasn't been seen yet\n",
    "                    fmap[fid] = 1\n",
    "                    gmap[fid] = g\n",
    "                else:\n",
    "                    fmap[fid] += 1\n",
    "                    gmap[fid] += g\n",
    "\n",
    "        # calculate average value (gain/cover) for each feature\n",
    "        for fid in gmap:\n",
    "            gmap[fid] = gmap[fid] / fmap[fid]\n",
    "\n",
    "        return gmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances_data = []\n",
    "features = X_test.columns\n",
    "for feature_name, feature_importance in get_importance(clf.best_estimator_.booster(), 'gain').iteritems():\n",
    "    feature_importances_data.append({\n",
    "            'feature': feature_name,\n",
    "            'importance': feature_importance\n",
    "        })\n",
    "\n",
    "feature_importances = pd.DataFrame(feature_importances_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances['abs_imp'] = feature_importances['importance'].apply(lambda x: abs(x))\n",
    "feature_importances_sort = feature_importances.sort_values(by='abs_imp',ascending=False)\n",
    "feature_importances_sort['relative_imp'] = 100.0 * (feature_importances_sort['abs_imp'] / feature_importances_sort['abs_imp'].max())\n",
    "feature_importances_sort = feature_importances_sort[::-1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "plt.title(\"Feature importances for Model\")\n",
    "plt.barh(feature_importances_sort.index, feature_importances_sort['relative_imp'],\n",
    "         color='#348ABD', align=\"center\", lw='3', edgecolor='#348ABD', alpha=0.6)\n",
    "plt.yticks(feature_importances_sort.index, feature_importances_sort['feature'], fontsize=12,)\n",
    "plt.ylim([-1, feature_importances_sort.index.max()+1])\n",
    "plt.xlim([0, feature_importances_sort['relative_imp'].max()*1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_cross_validation(_df, _classifier, _features_columns):\n",
    "    # cross validation type can be changed here\n",
    "    ss = sk.cross_validation.ShuffleSplit(len(_df.masking.unique()), n_iter=5, test_size=.3, random_state=0)\n",
    "    target='target'\n",
    "    prob_of = 'prob_of_all'\n",
    "    \n",
    "    results_cv_targeting = pd.DataFrame([], columns=['masking', target, 'fold', prob_of])\n",
    "\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    mean_lift = []\n",
    "    mean_tp = 0.0\n",
    "    mean_fp = range(0, 101)\n",
    "\n",
    "    nb_calls_cv = pd.DataFrame([],columns=['nb_contacts', 'total_population', 'total_pos_targets', 'nb_pos_targets', 'pos_rate', \n",
    "                                           'Percentage_of_pos_targets_found', 'Percentage_of_Population', 'Lift'])\n",
    "    feature_importances = pd.DataFrame([], columns=['feature', 'importance', 'fold'])\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 12))\n",
    "    fig.subplots_adjust(bottom=-0.5, left=-0.5, top=0.5, right=1.5)\n",
    "\n",
    "\n",
    "    print ('modeling started')\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(ss):\n",
    "\n",
    "        customer_id = _df.masking.unique().copy()\n",
    "        shuffled_customer_id = np.array(sorted(customer_id, key=lambda k: random.random()))\n",
    "        train_customer_id = shuffled_customer_id[train_index]\n",
    "        valid_customer_id = shuffled_customer_id[valid_index]\n",
    "\n",
    "        train = _df.loc[ _df.masking.isin(train_customer_id), np.concatenate([_features_columns, [target]],\n",
    "                        axis=0)].copy().reset_index(drop=True)\n",
    "        valid = _df.loc[_df.masking.isin(valid_customer_id), np.concatenate([_features_columns, [target]],\n",
    "                        axis=0)].copy().reset_index(drop=True)\n",
    "\n",
    "        temp = valid[['masking', target]].copy()\n",
    "        temp['fold'] = i\n",
    "\n",
    "        # modeling#\n",
    "        train_X = train.drop(['masking', target], axis=1)\n",
    "        valid_X = valid.drop(['masking', target], axis=1)\n",
    "\n",
    "        train_Y = np.array(train[target].astype(np.uint8))\n",
    "        valid_Y = np.array(valid[target].astype(np.uint8))\n",
    "\n",
    "        probas_ = _classifier.fit(train_X, train_Y,eval_metric='auc', eval_set=[(valid_X, valid_Y)],early_stopping_rounds=40).predict_proba(valid_X) \n",
    "        probabilities = pd.DataFrame(data=probas_[:, 1], index=valid_X.index, columns=[prob_of])\n",
    "\n",
    "        temp = temp.join(probabilities, how='left')\n",
    "        results_cv_targeting = results_cv_targeting.append(temp)\n",
    "\n",
    "        ###############################################################################\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = sk.metrics.roc_curve(valid_Y, probas_[:, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = sk.metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(fpr, tpr, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        ###############################################################################\n",
    "        # compute lift at 10%#\n",
    "        sorted_proba = np.array(list(reversed(np.argsort(probas_[:, 1]))))\n",
    "        X_test = valid_X\n",
    "        y_test = valid_Y\n",
    "        centile = X_test.shape[0] / 100\n",
    "        positives = sum(y_test)\n",
    "        lift = [0]\n",
    "        for q in xrange(1, 101):\n",
    "            if q == 100:\n",
    "                tp = sum(np.array(y_test)[sorted_proba[(q - 1) * X_test.shape[0] / 100:X_test.shape[0]]])\n",
    "            else:\n",
    "                tp = sum(\n",
    "                    np.array(y_test)[sorted_proba[(q - 1) * X_test.shape[0] / 100:q * X_test.shape[0] / 100]])\n",
    "            lift.append(lift[q - 1] + 100 * tp / float(positives))\n",
    "        quantiles = range(0, 101)\n",
    "        mean_tp += interp(mean_fp, mean_fp, lift)\n",
    "        mean_tp[0] = 0.0\n",
    "        mean_lift.append(lift[10] / 10.)\n",
    "\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(quantiles, lift, label='Lift fold %d at 10 = %0.2f' % (i, lift[10] / 10.))\n",
    "        print ('shuffle: %i, AUC: %f, lift at 10 percent: %f' % (i, roc_auc, lift[10] / 10.))\n",
    "        \n",
    "        ###############################################################################\n",
    "        # Calculate nb contacts to make\n",
    "        nb_calls = temp[['target','prob_of_all','fold']].copy()\n",
    "        nb_calls = nb_calls.sort_values(by='prob_of_all', ascending=False).reset_index(drop=True)\n",
    "        nb_calls['cum_Xsellers'] = np.cumsum(nb_calls.target)\n",
    "        nb_calls = nb_calls.reset_index(drop=False)\n",
    "        nb_calls = nb_calls.rename(columns={'index':'rank'})\n",
    "        nb_calls['nb_contacts_100'] = nb_calls.loc[nb_calls.cum_Xsellers==100,'rank'].min()\n",
    "        nb_calls['nb_contacts_200'] = nb_calls.loc[nb_calls.cum_Xsellers==200,'rank'].min()\n",
    "        nb_calls['nb_contacts_500'] = nb_calls.loc[nb_calls.cum_Xsellers==500,'rank'].min()\n",
    "        nb_calls['nb_contacts_1000'] = nb_calls.loc[nb_calls.cum_Xsellers==1000,'rank'].min()\n",
    "        nb_calls['nb_contacts_2000'] = nb_calls.loc[nb_calls.cum_Xsellers==2000,'rank'].min()\n",
    "        nb_calls['nb_contacts_3000'] = nb_calls.loc[nb_calls.cum_Xsellers==3000,'rank'].min()\n",
    "        nb_calls['nb_contacts_all'] = nb_calls.loc[nb_calls.cum_Xsellers==nb_calls.cum_Xsellers.max(),'rank'].min()\n",
    "        nb_calls = nb_calls[['nb_contacts_100','nb_contacts_200', 'nb_contacts_500','nb_contacts_1000', 'nb_contacts_2000','nb_contacts_3000','nb_contacts_all']].min()\n",
    "        nb_calls = pd.DataFrame(nb_calls,columns=['nb_contacts'])\n",
    "        nb_calls['total_population'] = temp.shape[0]\n",
    "        nb_calls['total_pos_targets'] = temp.target.sum()\n",
    "        nb_calls['nb_pos_targets']=[100,200,500,1000,2000,3000, temp.target.sum()]\n",
    "        nb_calls['pos_rate'] = nb_calls.nb_pos_targets/nb_calls.nb_contacts\n",
    "        nb_calls['Percentage_of_pos_targets_found'] = nb_calls.nb_pos_targets/nb_calls.total_pos_targets\n",
    "        nb_calls['Percentage_of_Population'] = nb_calls.nb_contacts/nb_calls.total_population\n",
    "        nb_calls['Lift'] = nb_calls.Percentage_of_pos_targets_found/nb_calls.Percentage_of_Population\n",
    "\n",
    "        nb_calls_cv = nb_calls_cv.append(nb_calls)\n",
    "        \n",
    "        ###############################################################################\n",
    "        feature_importances_data = []\n",
    "        features = train_X.columns\n",
    "        for feature_name, feature_importance in get_importance(_classifier.booster(), 'gain').iteritems():\n",
    "            feature_importances_data.append({\n",
    "                'feature': feature_name,\n",
    "                'importance': feature_importance\n",
    "            })\n",
    "\n",
    "        temp = pd.DataFrame(feature_importances_data)\n",
    "        temp['fold'] = i\n",
    "        feature_importances = feature_importances.append(temp)\n",
    "        \n",
    "    \n",
    "    nb_calls_cv = nb_calls_cv.reset_index().groupby('index').mean().sort_values(by='nb_pos_targets')\n",
    "    results_cv_targeting = results_cv_targeting.reset_index(drop=True)\n",
    "    \n",
    "    feature_importances = feature_importances.groupby('feature')['importance'].agg([np.mean, np.std])\n",
    "    feature_importances = feature_importances.sort_values(by='mean')\n",
    "    feature_importances = feature_importances.reset_index()\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    mean_tpr /= len(ss)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = sk.metrics.auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6))\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    mean_tp /= len(ss)\n",
    "    mean_tp[-1] = 100.0\n",
    "    mean_lift10 = np.mean(mean_lift)\n",
    "    print('Mean AUC: %f, Mean lift at 10 percent: %f' % (mean_auc, mean_lift10))\n",
    "    plt.plot(mean_fp, mean_tp, 'k--', label='Mean lift at 10 = %0.2f' % mean_lift10, lw=2)\n",
    "\n",
    "    plt.plot([0, 100], [0, 100], 'k--', color=(0.6, 0.6, 0.6))\n",
    "    plt.xlim([-5, 105])\n",
    "    plt.ylim([-5, 105])\n",
    "    plt.xlabel('Percentage of population')\n",
    "    plt.ylabel('Cumulative gain')\n",
    "    plt.title('Lift', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return results_cv_targeting, feature_importances, nb_calls_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters of the classifier need to be changed according to datasets\n",
    "classifier = xgb.XGBClassifier(objective='binary:logistic',max_depth=6,n_estimators=300, learning_rate=0.05,max_delta_step=1,\n",
    "                        min_child_weight=25, gamma=0.1, scale_pos_weight=1, colsample_bytree=0.85, subsample=0.85,colsample_bylevel=0.85,\n",
    "                        nthread=10, seed=27)\n",
    "\n",
    "results_cv_targeting, feature_importances, nb_calls_cv = run_cross_validation(dataset_modelA_clean_targeting, classifier , feature_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances['abs_imp'] = feature_importances['mean'].apply(lambda x: abs(x))\n",
    "feature_importances_sort = feature_importances.sort_values(by='abs_imp',ascending=False)\n",
    "feature_importances_sort['relative_imp'] = 100.0 * (feature_importances_sort['abs_imp'] / feature_importances_sort['abs_imp'].max())\n",
    "feature_importances_sort = feature_importances_sort[::-1].reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.title(\"Feature importances for Model\")\n",
    "plt.barh(feature_importances_sort.index, feature_importances_sort['relative_imp'],\n",
    "         color='#348ABD', align=\"center\", lw='3', edgecolor='#348ABD', alpha=0.6)\n",
    "plt.yticks(feature_importances_sort.index, feature_importances_sort['feature'], fontsize=12,)\n",
    "plt.ylim([-1, feature_importances_sort.index.max()+1])\n",
    "plt.xlim([0, feature_importances_sort['relative_imp'].max()*1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Choose thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_cross_validation(_df, _classifier, _features_columns):\n",
    "    # cross validation type can be changed here\n",
    "    kf = KFold(len(_df), n_folds=3, shuffle=True)\n",
    "    target='target'\n",
    "    prob_of = 'prob_of_all'\n",
    "    \n",
    "    results_cv_targeting = pd.DataFrame([], columns=['masking', target, 'fold', prob_of])\n",
    "\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    mean_lift = []\n",
    "    mean_tp = 0.0\n",
    "    mean_fp = range(0, 101)\n",
    "\n",
    "    nb_calls_cv = pd.DataFrame([],columns=['nb_contacts', 'total_population', 'total_pos_targets', 'nb_pos_targets', 'pos_rate', \n",
    "                                           'Percentage_of_pos_targets_found', 'Percentage_of_Population', 'Lift'])\n",
    "    feature_importances = pd.DataFrame([], columns=['feature', 'importance', 'fold'])\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 12))\n",
    "    fig.subplots_adjust(bottom=-0.5, left=-0.5, top=0.5, right=1.5)\n",
    "\n",
    "    X = _df[np.concatenate([_features_columns,[target]])]\n",
    "    y = _df[target]\n",
    "    \n",
    "    print ('modeling started')\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(kf):\n",
    "        \n",
    "        train_X, valid_X = X.iloc[train_index], X.iloc[valid_index]\n",
    "        train_Y, valid_Y = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "        temp = valid_X[['masking', target]].copy()\n",
    "        temp['fold'] = i\n",
    "\n",
    "        # modeling#\n",
    "        train_X = train_X.drop(['masking', target], axis=1)\n",
    "        valid_X = valid_X.drop(['masking', target], axis=1)\n",
    "\n",
    "        train_Y = np.array(train_Y.astype(np.uint8))\n",
    "        valid_Y = np.array(valid_Y.astype(np.uint8))\n",
    "        \n",
    "\n",
    "\n",
    "        probas_ = _classifier.fit(train_X, train_Y).predict_proba(valid_X)\n",
    "        probabilities = pd.DataFrame(data=probas_[:, 1], index=valid_X.index, columns=[prob_of])\n",
    "\n",
    "        temp = temp.join(probabilities, how='left')\n",
    "        results_cv_targeting = results_cv_targeting.append(temp)\n",
    "\n",
    "        ###############################################################################\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = sk.metrics.roc_curve(valid_Y, probas_[:, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = sk.metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(fpr, tpr, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        ###############################################################################\n",
    "        # compute lift at 10%#\n",
    "        sorted_proba = np.array(list(reversed(np.argsort(probas_[:, 1]))))\n",
    "        X_test = valid_X\n",
    "        y_test = valid_Y\n",
    "        centile = X_test.shape[0] / 100\n",
    "        positives = sum(y_test)\n",
    "        lift = [0]\n",
    "        for q in xrange(1, 101):\n",
    "            if q == 100:\n",
    "                tp = sum(np.array(y_test)[sorted_proba[(q - 1) * X_test.shape[0] / 100:X_test.shape[0]]])\n",
    "            else:\n",
    "                tp = sum(\n",
    "                    np.array(y_test)[sorted_proba[(q - 1) * X_test.shape[0] / 100:q * X_test.shape[0] / 100]])\n",
    "            lift.append(lift[q - 1] + 100 * tp / float(positives))\n",
    "        quantiles = range(0, 101)\n",
    "        mean_tp += interp(mean_fp, mean_fp, lift)\n",
    "        mean_tp[0] = 0.0\n",
    "        mean_lift.append(lift[10] / 10.)\n",
    "\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(quantiles, lift, label='Lift fold %d at 10 = %0.2f' % (i, lift[10] / 10.))\n",
    "        print ('shuffle: %i, AUC: %f, lift at 10 percent: %f' % (i, roc_auc, lift[10] / 10.))\n",
    "        \n",
    "        ###############################################################################\n",
    "        # Calculate nb contacts to make\n",
    "        nb_calls = temp[['target','prob_of_all','fold']].copy()\n",
    "        nb_calls = nb_calls.sort_values(by='prob_of_all', ascending=False).reset_index(drop=True)\n",
    "        nb_calls['cum_Xsellers'] = np.cumsum(nb_calls.target)\n",
    "        nb_calls = nb_calls.reset_index(drop=False)\n",
    "        nb_calls = nb_calls.rename(columns={'index':'rank'})\n",
    "        nb_calls['nb_contacts_100'] = nb_calls.loc[nb_calls.cum_Xsellers==100,'rank'].min()\n",
    "        nb_calls['nb_contacts_200'] = nb_calls.loc[nb_calls.cum_Xsellers==200,'rank'].min()\n",
    "        nb_calls['nb_contacts_500'] = nb_calls.loc[nb_calls.cum_Xsellers==500,'rank'].min()\n",
    "        nb_calls['nb_contacts_1000'] = nb_calls.loc[nb_calls.cum_Xsellers==1000,'rank'].min()\n",
    "        nb_calls['nb_contacts_2000'] = nb_calls.loc[nb_calls.cum_Xsellers==2000,'rank'].min()\n",
    "        nb_calls['nb_contacts_3000'] = nb_calls.loc[nb_calls.cum_Xsellers==3000,'rank'].min()\n",
    "        nb_calls['nb_contacts_all'] = nb_calls.loc[nb_calls.cum_Xsellers==nb_calls.cum_Xsellers.max(),'rank'].min()\n",
    "        nb_calls = nb_calls[['nb_contacts_100','nb_contacts_200', 'nb_contacts_500','nb_contacts_1000', 'nb_contacts_2000','nb_contacts_3000','nb_contacts_all']].min()\n",
    "        nb_calls = pd.DataFrame(nb_calls,columns=['nb_contacts'])\n",
    "        nb_calls['total_population'] = temp.shape[0]\n",
    "        nb_calls['total_pos_targets'] = temp.target.sum()\n",
    "        nb_calls['nb_pos_targets']=[100,200,500,1000,2000,3000, temp.target.sum()]\n",
    "        nb_calls['pos_rate'] = nb_calls.nb_pos_targets/nb_calls.nb_contacts\n",
    "        nb_calls['Percentage_of_pos_targets_found'] = nb_calls.nb_pos_targets/nb_calls.total_pos_targets\n",
    "        nb_calls['Percentage_of_Population'] = nb_calls.nb_contacts/nb_calls.total_population\n",
    "        nb_calls['Lift'] = nb_calls.Percentage_of_pos_targets_found/nb_calls.Percentage_of_Population\n",
    "\n",
    "        nb_calls_cv = nb_calls_cv.append(nb_calls)\n",
    "        \n",
    "        ###############################################################################\n",
    "        feature_importances_data = []\n",
    "        features = train_X.columns\n",
    "        for feature_name, feature_importance in get_importance(_classifier.booster(), 'gain').iteritems():\n",
    "            feature_importances_data.append({\n",
    "                'feature': feature_name,\n",
    "                'importance': feature_importance\n",
    "            })\n",
    "\n",
    "        temp = pd.DataFrame(feature_importances_data)\n",
    "        temp['fold'] = i\n",
    "        feature_importances = feature_importances.append(temp)\n",
    "    \n",
    "    nb_calls_cv = nb_calls_cv.reset_index().groupby('index').mean().sort_values(by='nb_pos_targets')\n",
    "    nb_calls_cv['total_population'] = nb_calls_cv['total_population'].apply(lambda x : round(x,0))\n",
    "    results_cv_targeting = results_cv_targeting.reset_index(drop=True)\n",
    "    \n",
    "    feature_importances = feature_importances.groupby('feature')['importance'].agg([np.mean, np.std])\n",
    "    feature_importances = feature_importances.sort_values(by='mean')\n",
    "    feature_importances = feature_importances.reset_index()\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    mean_tpr /= len(kf)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = sk.metrics.auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6))\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    mean_tp /= len(kf)\n",
    "    mean_tp[-1] = 100.0\n",
    "    mean_lift10 = np.mean(mean_lift)\n",
    "    print('Mean AUC: %f, Mean lift at 10 percent: %f' % (mean_auc, mean_lift10))\n",
    "    plt.plot(mean_fp, mean_tp, 'k--', label='Mean lift at 10 = %0.2f' % mean_lift10, lw=2)\n",
    "\n",
    "    plt.plot([0, 100], [0, 100], 'k--', color=(0.6, 0.6, 0.6))\n",
    "    plt.xlim([-5, 105])\n",
    "    plt.ylim([-5, 105])\n",
    "    plt.xlabel('Percentage of population')\n",
    "    plt.ylabel('Cumulative gain')\n",
    "    plt.title('Lift', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return results_cv_targeting, feature_importances, nb_calls_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters of the classifier need to be changed according to datasets\n",
    "classifier = xgb.XGBClassifier(objective='binary:logistic',max_depth=6,n_estimators=200, learning_rate=0.05,max_delta_step=1,\n",
    "                        min_child_weight=25, gamma=0.1, scale_pos_weight=1, colsample_bytree=0.85, subsample=0.85,colsample_bylevel=0.85,\n",
    "                        nthread=10, seed=27)\n",
    "\n",
    "results_cv_targeting, feature_importances, nb_calls_cv = run_cross_validation(dataset_modelA_clean_targeting, classifier , feature_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_calls_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_calls_all_cv = pd.DataFrame([],columns=['nb_contacts', 'total_population', 'total_pos_targets', 'nb_pos_targets', 'pos_rate', \n",
    "                                           'Percentage_of_pos_targets_found', 'Percentage_of_Population', 'Lift'])\n",
    "\n",
    "# Calculate nb calls to make\n",
    "nb_calls_all = results_cv_targeting[['target', 'prob_of_all']].copy()\n",
    "nb_calls_all = nb_calls_all.sort_values(by='prob_of_all', ascending=False).reset_index(drop=True)\n",
    "nb_calls_all['cum_Xsellers'] = np.cumsum(nb_calls_all.target)\n",
    "nb_calls_all = nb_calls_all.reset_index(drop=False)\n",
    "nb_calls_all = nb_calls_all.rename(columns={'index': 'rank'})\n",
    "nb_calls_all['nb_calls_100'] = nb_calls_all.loc[nb_calls_all.cum_Xsellers == 100, 'rank'].min()\n",
    "nb_calls_all['nb_calls_200'] = nb_calls_all.loc[nb_calls_all.cum_Xsellers == 200, 'rank'].min()\n",
    "nb_calls_all['nb_calls_500'] = nb_calls_all.loc[nb_calls_all.cum_Xsellers == 500, 'rank'].min()\n",
    "nb_calls_all['nb_calls_1000'] = nb_calls_all.loc[nb_calls_all.cum_Xsellers == 1000, 'rank'].min()\n",
    "nb_calls_all['nb_calls_2000'] = nb_calls_all.loc[nb_calls_all.cum_Xsellers == 2000, 'rank'].min()\n",
    "nb_calls_all['nb_calls_3000'] = nb_calls_all.loc[nb_calls_all.cum_Xsellers == 3000, 'rank'].min()\n",
    "nb_calls_all['nb_calls_all'] = nb_calls_all.loc[nb_calls_all.cum_Xsellers == nb_calls_all.cum_Xsellers.max(), 'rank'].min()\n",
    "nb_calls_all = nb_calls_all[\n",
    "    ['nb_calls_100', 'nb_calls_200', 'nb_calls_500', 'nb_calls_1000', 'nb_calls_2000', 'nb_calls_3000',\n",
    "     'nb_calls_all']].min()\n",
    "nb_calls_all = pd.DataFrame(nb_calls_all, columns=['nb_contacts'])\n",
    "nb_calls_all['total_population'] = results_cv_targeting.shape[0]\n",
    "nb_calls_all['total_pos_targets'] = results_cv_targeting.target.sum()\n",
    "nb_calls_all['nb_pos_targets']=[100,200,500,1000,2000,3000, results_cv_targeting.target.sum()]\n",
    "nb_calls_all['pos_rate'] = nb_calls_all.nb_pos_targets/nb_calls_all.nb_contacts\n",
    "nb_calls_all['Percentage_of_pos_targets_found'] = nb_calls_all.nb_pos_targets/nb_calls_all.total_pos_targets\n",
    "nb_calls_all['Percentage_of_Population'] = nb_calls_all.nb_contacts/nb_calls_all.total_population\n",
    "nb_calls_all['Lift'] = nb_calls_all.Percentage_of_pos_targets_found/nb_calls_all.Percentage_of_Population\n",
    "\n",
    "\n",
    "nb_calls_all_cv = nb_calls_all_cv.append(nb_calls_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_calls_all_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##objective for having 500 positive targets\n",
    "\n",
    "p1 = results_cv_targeting.sort_values(by='prob_of_all',ascending=False).reset_index(drop=True)\n",
    "p1['nb_pos_target']=np.cumsum(p1.target)\n",
    "index = p1.loc[p1.nb_pos_target==500].index.min()\n",
    "print('min number to contact %f to get 500 Xsellers'%(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##objective for taking maximum of agents capability (1/10 per week, min conversion rate 10%)\n",
    "\n",
    "r1 = results_cv_targeting.sort_values(by='prob_of_all',ascending=False).reset_index(drop=True)\n",
    "r1['nb_pos_target'] = np.cumsum(r1.target)\n",
    "r1 = r1.reset_index(drop=False)\n",
    "r1 = r1.rename(columns={'index':'nb_contact'})\n",
    "r1.nb_contact = r1.nb_contact+1\n",
    "r1['pos_rate'] = r1['nb_pos_target'] / r1['nb_contact']\n",
    "index = r1[(r1.pos_rate>0.1)].index.max()\n",
    "print('max nb to contact %f customers to guarantee 10percente conversion rate'%(index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
